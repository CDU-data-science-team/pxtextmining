{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home This site contains the project documentation for the pxtextmining python package. This provides a technical overview of the package; for a non-technical overview and further information, visit the Patient Experience Qualitative Data Categorisation website . Table Of Contents The documentation is split into three separate sections: Project background Getting started, a simple approach to using the package: Installation How the package works Training a new model Making predictions with a trained model Code reference, a more technical overview of the functions and modules: Factories Helpers Pipelines","title":"Home"},{"location":"#home","text":"This site contains the project documentation for the pxtextmining python package. This provides a technical overview of the package; for a non-technical overview and further information, visit the Patient Experience Qualitative Data Categorisation website .","title":"Home"},{"location":"#table-of-contents","text":"The documentation is split into three separate sections: Project background Getting started, a simple approach to using the package: Installation How the package works Training a new model Making predictions with a trained model Code reference, a more technical overview of the functions and modules: Factories Helpers Pipelines","title":"Table Of Contents"},{"location":"about/","text":"Project background The pxtextmining package is part of the Patient Experience Qualitative Data Categorisation project . This project is is hosted by Nottinghamshire Healthcare NHS Foundation Trust's Clinical Development Unit Data Science Team, and funded by NHS England's Insight and Feedback Team. The primary objective of the pxtextmining element is to create a machine learning model capable of categorising the free text data obtained through the NHS England Friends and Family Test (FFT). It is a multilabel classification problem, with one or more categories applied to each patient feedback comment. In this way, we hope to support better use of qualitative patient experience feedback by NHS provider organisations. This package works together with the experiencesdashboard , a frontend coded in R/Shiny.","title":"Project background"},{"location":"about/#project-background","text":"The pxtextmining package is part of the Patient Experience Qualitative Data Categorisation project . This project is is hosted by Nottinghamshire Healthcare NHS Foundation Trust's Clinical Development Unit Data Science Team, and funded by NHS England's Insight and Feedback Team. The primary objective of the pxtextmining element is to create a machine learning model capable of categorising the free text data obtained through the NHS England Friends and Family Test (FFT). It is a multilabel classification problem, with one or more categories applied to each patient feedback comment. In this way, we hope to support better use of qualitative patient experience feedback by NHS provider organisations. This package works together with the experiencesdashboard , a frontend coded in R/Shiny.","title":"Project background"},{"location":"getting%20started/install/","text":"Installation You can install pxtextmining from either PyPI or GitHub . The recommended method is to clone the repository from GitHub, as this will also include the models and datasets. Option 1: Install from PyPI This option allows you to use the functions coded in pxtextmining. Install pxtextmining and its PyPI dependencies: pip install pxtextmining Option 2 (RECOMMENDED): Install from GitHub This option is recommended as it gives you access to the full datasets and already trained models. To begin with, clone the repository from github . It is also recommended to create a new virtual environment , using your chosen method of managing Python environments. The package uses poetry for dependency management. First, run pip install poetry . Then, run poetry install --with dev .","title":"Install"},{"location":"getting%20started/install/#installation","text":"You can install pxtextmining from either PyPI or GitHub . The recommended method is to clone the repository from GitHub, as this will also include the models and datasets.","title":"Installation"},{"location":"getting%20started/install/#option-1-install-from-pypi","text":"This option allows you to use the functions coded in pxtextmining. Install pxtextmining and its PyPI dependencies: pip install pxtextmining","title":"Option 1: Install from PyPI"},{"location":"getting%20started/install/#option-2-recommended-install-from-github","text":"This option is recommended as it gives you access to the full datasets and already trained models. To begin with, clone the repository from github . It is also recommended to create a new virtual environment , using your chosen method of managing Python environments. The package uses poetry for dependency management. First, run pip install poetry . Then, run poetry install --with dev .","title":"Option 2 (RECOMMENDED): Install from GitHub"},{"location":"getting%20started/package/","text":"Package structure pxtextmining The pxtextmining package is constructed using the following elements: pxtextmining.factories This module contains vast majority of the code in the package. There are five different stages, each corresponding to a different submodule. factory_data_load_and_split : Loading of multilabel data, preprocessing, and splitting into train/test/validation sets as appropriate. factory_pipeline : Construction and training of different models/estimators/algorithms using the sklearn , tensorflow.keras and transformers libraries. factory_model_performance : Evaluation of a trained model, comparing predicted targets with real target values, to produce performance metrics. The decision-making process behind the peformance metrics chosen can be seen on the project documentation website . The performance metrics for the current best models utilised in the API can be found in the current_best_multilabel folder in the main repository. factory_predict_unlabelled_text : Prepares unlabelled text (with or without additional features such as question type) in a format suitable for each model type, and passes this through the selected models, to produce predicted labels. pxtextmining.helpers This module contains some helper functions which are used in pxtextmining.factories . Some of this is legacy code, so this may just be moved into the factories submodule in future versions of the package. pxtextmining.pipelines All of the processes in pxtextmining.factories are pulled together in multilabel_pipeline , to create the complete end-to-end process of data processing, model creation, training, evaluation, and saving. There is also a pxtextmining.params file which is used to standardise specific variables that are used across the entire package. The aim of this is to reduce repetition across the package, for example when trying different targets or model types. API Separate from the pxtextmining package is the API, which can be found in the folder api . It is constructed using FastAPI and Uvicorn. The aim of the API is to make the trained machine learning models available publicly, so that predictions can be made on any text. The API is not currently publicly available and access is only for participating partner trusts. However, all the code and documentation is available on our github repository.","title":"Package structure"},{"location":"getting%20started/package/#package-structure","text":"","title":"Package structure"},{"location":"getting%20started/package/#pxtextmining","text":"The pxtextmining package is constructed using the following elements: pxtextmining.factories This module contains vast majority of the code in the package. There are five different stages, each corresponding to a different submodule. factory_data_load_and_split : Loading of multilabel data, preprocessing, and splitting into train/test/validation sets as appropriate. factory_pipeline : Construction and training of different models/estimators/algorithms using the sklearn , tensorflow.keras and transformers libraries. factory_model_performance : Evaluation of a trained model, comparing predicted targets with real target values, to produce performance metrics. The decision-making process behind the peformance metrics chosen can be seen on the project documentation website . The performance metrics for the current best models utilised in the API can be found in the current_best_multilabel folder in the main repository. factory_predict_unlabelled_text : Prepares unlabelled text (with or without additional features such as question type) in a format suitable for each model type, and passes this through the selected models, to produce predicted labels. pxtextmining.helpers This module contains some helper functions which are used in pxtextmining.factories . Some of this is legacy code, so this may just be moved into the factories submodule in future versions of the package. pxtextmining.pipelines All of the processes in pxtextmining.factories are pulled together in multilabel_pipeline , to create the complete end-to-end process of data processing, model creation, training, evaluation, and saving. There is also a pxtextmining.params file which is used to standardise specific variables that are used across the entire package. The aim of this is to reduce repetition across the package, for example when trying different targets or model types.","title":"pxtextmining"},{"location":"getting%20started/package/#api","text":"Separate from the pxtextmining package is the API, which can be found in the folder api . It is constructed using FastAPI and Uvicorn. The aim of the API is to make the trained machine learning models available publicly, so that predictions can be made on any text. The API is not currently publicly available and access is only for participating partner trusts. However, all the code and documentation is available on our github repository.","title":"API"},{"location":"getting%20started/training_new_model/","text":"Training a new model To train a new model to categorise patient feedback text, labelled data is required. Discussions are currently underway to enable the release of the data that the multilabel models in pxtextmining are trained on. This page breaks down the steps in the function pxtextmining.pipelines.run_sklearn_pipeline , which outputs trained sklearn models. This is a high-level explanation of the processes; for more detailed technical information please see the relevant code reference pages for each function. # Step 1: Generate a random_state which is used for the train_test_split. # This means that the pipeline and evaluation should be reproducible. random_state = random.randint(1,999) # Step 2: Load the data and isolate the target columns from the dataframe. df = load_multilabel_data(filename = 'datasets/hidden/multilabeldata_2.csv', target = 'major_categories') # Step 3: Conduct preprocessing: remove punctuation and numbers, clean whitespace and drop empty lines. # Split into train and test using the random_state above. X_train, X_test, Y_train, Y_test = process_and_split_data( df, target = target, random_state = random_state) # Step 4: Instantiate a pipeline and hyperparamter grid for each estimator to be tried. # Conduct a cross-validated randomized search to identify the hyperparameters # producing the best results on the validation set. # For each estimator, returns the pipeline with the best hyperparameters, # together with the time taken to search the pipeline. models, training_times = search_sklearn_pipelines(X_train, Y_train, models_to_try = models_to_try, additional_features = additional_features) # Step 5: Evaluate each pipeline using the test set, comparing predicted values with real values. # Performance metrics are recorded together with the time taken to search the pipeline. model_metrics = [] for i in range(len(models)): m = models[i] t = training_times[i] model_metrics.append(get_multilabel_metrics(X_test, Y_test, random_state = random_state, labels = target, model_type = 'sklearn', model = m, training_time = t)) # Step 6: Save the models and performance metrics to the path specified write_multilabel_models_and_metrics(models,model_metrics,path=path)","title":"Training a new model"},{"location":"getting%20started/training_new_model/#training-a-new-model","text":"To train a new model to categorise patient feedback text, labelled data is required. Discussions are currently underway to enable the release of the data that the multilabel models in pxtextmining are trained on. This page breaks down the steps in the function pxtextmining.pipelines.run_sklearn_pipeline , which outputs trained sklearn models. This is a high-level explanation of the processes; for more detailed technical information please see the relevant code reference pages for each function. # Step 1: Generate a random_state which is used for the train_test_split. # This means that the pipeline and evaluation should be reproducible. random_state = random.randint(1,999) # Step 2: Load the data and isolate the target columns from the dataframe. df = load_multilabel_data(filename = 'datasets/hidden/multilabeldata_2.csv', target = 'major_categories') # Step 3: Conduct preprocessing: remove punctuation and numbers, clean whitespace and drop empty lines. # Split into train and test using the random_state above. X_train, X_test, Y_train, Y_test = process_and_split_data( df, target = target, random_state = random_state) # Step 4: Instantiate a pipeline and hyperparamter grid for each estimator to be tried. # Conduct a cross-validated randomized search to identify the hyperparameters # producing the best results on the validation set. # For each estimator, returns the pipeline with the best hyperparameters, # together with the time taken to search the pipeline. models, training_times = search_sklearn_pipelines(X_train, Y_train, models_to_try = models_to_try, additional_features = additional_features) # Step 5: Evaluate each pipeline using the test set, comparing predicted values with real values. # Performance metrics are recorded together with the time taken to search the pipeline. model_metrics = [] for i in range(len(models)): m = models[i] t = training_times[i] model_metrics.append(get_multilabel_metrics(X_test, Y_test, random_state = random_state, labels = target, model_type = 'sklearn', model = m, training_time = t)) # Step 6: Save the models and performance metrics to the path specified write_multilabel_models_and_metrics(models,model_metrics,path=path)","title":"Training a new model"},{"location":"getting%20started/using_trained_model/","text":"Using a trained model The current_best_multilabel folder should contain a fully trained sklearn model in .sav format, as well as performance metrics for the model. The Transformer-based tensorflow.keras model is over 1GB and cannot be shared via GitHub. However, it will be made available via the API, which is forthcoming in a future release of this package. This page breaks down the steps in the function pxtextmining.pipelines.factory_predict_unlabelled_text.predict_multilabel_sklearn , which can make predictions using the sklearn model available via GitHub. This is a high-level explanation of the processes; for more detailed technical information please see the relevant code reference page. # Step 1: Conduct preprocessing on text: # Temove trailing whitespaces, NULL values, NaNs, and punctuation. Converts to lowercase. text_no_whitespace = text.replace(r\"^\\s*$\", np.nan, regex=True) text_no_nans = text_no_whitespace.dropna() text_cleaned = text_no_nans.astype(str).apply(remove_punc_and_nums) processed_text = text_cleaned.astype(str).apply(clean_empty_features) # Step 2: Make predictions with the trained model binary_preds = model.predict(processed_text) # Step 3: Get predicted probabilities for each label pred_probs = np.array(model.predict_proba(processed_text)) # Step 4: Some samples do not have any predicted labels. # For these, take the label with the highest predicted probability. predictions = fix_no_labels(binary_preds, pred_probs, model_type=\"sklearn\") # Step 5: Convert predictions to a dataframe. preds_df = pd.DataFrame(predictions, index=processed_text.index, columns=labels) preds_df[\"labels\"] = preds_df.apply(get_labels, args=(labels,), axis=1)","title":"Using a trained model"},{"location":"getting%20started/using_trained_model/#using-a-trained-model","text":"The current_best_multilabel folder should contain a fully trained sklearn model in .sav format, as well as performance metrics for the model. The Transformer-based tensorflow.keras model is over 1GB and cannot be shared via GitHub. However, it will be made available via the API, which is forthcoming in a future release of this package. This page breaks down the steps in the function pxtextmining.pipelines.factory_predict_unlabelled_text.predict_multilabel_sklearn , which can make predictions using the sklearn model available via GitHub. This is a high-level explanation of the processes; for more detailed technical information please see the relevant code reference page. # Step 1: Conduct preprocessing on text: # Temove trailing whitespaces, NULL values, NaNs, and punctuation. Converts to lowercase. text_no_whitespace = text.replace(r\"^\\s*$\", np.nan, regex=True) text_no_nans = text_no_whitespace.dropna() text_cleaned = text_no_nans.astype(str).apply(remove_punc_and_nums) processed_text = text_cleaned.astype(str).apply(clean_empty_features) # Step 2: Make predictions with the trained model binary_preds = model.predict(processed_text) # Step 3: Get predicted probabilities for each label pred_probs = np.array(model.predict_proba(processed_text)) # Step 4: Some samples do not have any predicted labels. # For these, take the label with the highest predicted probability. predictions = fix_no_labels(binary_preds, pred_probs, model_type=\"sklearn\") # Step 5: Convert predictions to a dataframe. preds_df = pd.DataFrame(predictions, index=processed_text.index, columns=labels) preds_df[\"labels\"] = preds_df.apply(get_labels, args=(labels,), axis=1)","title":"Using a trained model"},{"location":"reference/API/API/","text":"pxtextmining API overview We have created two different APIs for labelling patient experience feedback. Both APIs are free to use and completely open source. For help and support with using them, please contact Chris Beeley . The \"Quick API\" is faster and simpler, as it uses an sklearn model which is quicker to make predictions. The performance of predictions from this API can be seen on our project documentation website. It is less accurate than the slow API. This API is a more 'traditional' style of API. The \"Slow API\" utilises sklearn models as well as the slower but more powerful transformer-based Distilbert model. Due to the demanding hardware requirements of this model, we have set up a slower and slightly more complex API which combines (ensembles) together these models but has higher performance overall. Security The data is submitted via a secure HTTPS connection. All data is encrypted in transit with HTTPS, using the SSL/TLS protocol for encryption and authentication. The data is stored in blob storage on a UK-based Azure container instance for the duration of the model predictions, and is then immediately deleted. Ad hoc support is provided where possible, no uptime or other guarantees exist.","title":"pxtextmining API overview"},{"location":"reference/API/API/#pxtextmining-api-overview","text":"We have created two different APIs for labelling patient experience feedback. Both APIs are free to use and completely open source. For help and support with using them, please contact Chris Beeley . The \"Quick API\" is faster and simpler, as it uses an sklearn model which is quicker to make predictions. The performance of predictions from this API can be seen on our project documentation website. It is less accurate than the slow API. This API is a more 'traditional' style of API. The \"Slow API\" utilises sklearn models as well as the slower but more powerful transformer-based Distilbert model. Due to the demanding hardware requirements of this model, we have set up a slower and slightly more complex API which combines (ensembles) together these models but has higher performance overall.","title":"pxtextmining API overview"},{"location":"reference/API/API/#security","text":"The data is submitted via a secure HTTPS connection. All data is encrypted in transit with HTTPS, using the SSL/TLS protocol for encryption and authentication. The data is stored in blob storage on a UK-based Azure container instance for the duration of the model predictions, and is then immediately deleted. Ad hoc support is provided where possible, no uptime or other guarantees exist.","title":"Security"},{"location":"reference/API/quick_API/","text":"Quick API To facilitate the use of the models trained in this project, an API has been created using the FastAPI library. Users will be able to send their patient experience feedback comments to the model via the API, and will receive the predicted labels for those comments. This API utilises the Support Vector Classifier model which is less performant than the transformer-based Distilbert model. However, it is also much quicker and simpler. Performance metrics for this model can be seen on our project documentation website . The API has been created using FastAPI and is deployed on Posit Connect. The URL is available on request. Full documentation for the API, automatically generated by FastAPI, is available at [API URL]/docs. How to make an API call 1. Prepare the data in JSON format. In Python, this is a list containing as many dict s as there are comments to be predicted. Each dict has two compulsory keys: comment_id : Unique ID associated with the comment, in str format. Each Comment ID per API call must be unique. comment_text : Text to be classified, in str format. # In Python text_data = [ { 'comment_id': '1', # The comment_id values in each dict must be unique. 'comment_text': 'This is the first comment. Nurse was great.', }, { 'comment_id': '2', 'comment_text': 'This is the second comment. The ward was freezing.', }, { 'comment_id': '3', 'comment_text': '', # This comment is an empty string. }, ] # In R library(jsonlite) comment_id <- c(\"1\", \"2\", \"3\") comment_text <- c( \"This is the first comment. Nurse was great.\", \"This is the second comment. The ward was freezing.\", \"\" ) df <- data.frame(comment_id, comment_text) text_data <- toJSON(df) 2. Send the JSON containing the text data to the predict_multilabel endpoint. In python, this can be done using the requests library. # In Python import requests url = \"API_URL_GOES_HERE\" response = requests.post(f\"{url}/predict_multilabel\", json = text_data) # In R library(httr) r <- POST( url = \"API_URL_GOES_HERE\", body = text_data, encode = \"json\", add_headers( \"Content-Type\" = \"application/json\" ) ) 3. After waiting for the data to be processed and passed through the machine learning model, receive predicted labels at the same endpoint, in the example format below. Note that the comment with blank text, with comment_id 3, was assigned the label 'Labelling not possible' as it would have been stripped out during preprocessing. # In Python print(response.json()) # Output below [ { 'comment_id': '1', 'labels': ['Non-specific praise for staff']} , { 'comment_id': '2', 'labels': ['Sensory experience']} , { 'comment_id': '3', 'labels': ['Labelling not possible'] } ] # In R r_parsed = fromJSON(content(r, \"text\"))","title":"Quick API"},{"location":"reference/API/quick_API/#quick-api","text":"To facilitate the use of the models trained in this project, an API has been created using the FastAPI library. Users will be able to send their patient experience feedback comments to the model via the API, and will receive the predicted labels for those comments. This API utilises the Support Vector Classifier model which is less performant than the transformer-based Distilbert model. However, it is also much quicker and simpler. Performance metrics for this model can be seen on our project documentation website . The API has been created using FastAPI and is deployed on Posit Connect. The URL is available on request. Full documentation for the API, automatically generated by FastAPI, is available at [API URL]/docs.","title":"Quick API"},{"location":"reference/API/quick_API/#how-to-make-an-api-call","text":"1. Prepare the data in JSON format. In Python, this is a list containing as many dict s as there are comments to be predicted. Each dict has two compulsory keys: comment_id : Unique ID associated with the comment, in str format. Each Comment ID per API call must be unique. comment_text : Text to be classified, in str format. # In Python text_data = [ { 'comment_id': '1', # The comment_id values in each dict must be unique. 'comment_text': 'This is the first comment. Nurse was great.', }, { 'comment_id': '2', 'comment_text': 'This is the second comment. The ward was freezing.', }, { 'comment_id': '3', 'comment_text': '', # This comment is an empty string. }, ] # In R library(jsonlite) comment_id <- c(\"1\", \"2\", \"3\") comment_text <- c( \"This is the first comment. Nurse was great.\", \"This is the second comment. The ward was freezing.\", \"\" ) df <- data.frame(comment_id, comment_text) text_data <- toJSON(df) 2. Send the JSON containing the text data to the predict_multilabel endpoint. In python, this can be done using the requests library. # In Python import requests url = \"API_URL_GOES_HERE\" response = requests.post(f\"{url}/predict_multilabel\", json = text_data) # In R library(httr) r <- POST( url = \"API_URL_GOES_HERE\", body = text_data, encode = \"json\", add_headers( \"Content-Type\" = \"application/json\" ) ) 3. After waiting for the data to be processed and passed through the machine learning model, receive predicted labels at the same endpoint, in the example format below. Note that the comment with blank text, with comment_id 3, was assigned the label 'Labelling not possible' as it would have been stripped out during preprocessing. # In Python print(response.json()) # Output below [ { 'comment_id': '1', 'labels': ['Non-specific praise for staff']} , { 'comment_id': '2', 'labels': ['Sensory experience']} , { 'comment_id': '3', 'labels': ['Labelling not possible'] } ] # In R r_parsed = fromJSON(content(r, \"text\"))","title":"How to make an API call"},{"location":"reference/API/slow_API/","text":"Slow API This API is slower but uses the best performing models. The transformer-based Distilbert model consumes a lot of hardware resource, and as such required a different approach. For predicting the multilabel categories, the API endpoint ensembles together Support Vector Classifier, Gradient Boosted Decision Trees (XGBoost), and Distilbert models. For predicting text sentiment , the API endpoint utilises a Distilbert model. The API URL endpoint is available on request. You will need an API key, please contact the project team to obtain one. The key should be passed as a code param with your API request. How to make an API call 1. Prepare the data in JSON format. In Python, this is a list containing as many dict s as there are comments to be predicted. Each dict has three compulsory keys: comment_id : Unique ID associated with the comment, in str format. Each Comment ID per API call must be unique. comment_text : Text to be classified, in str format. question_type : The type of question asked to elicit the comment text. Questions are different from trust to trust, but they all fall into one of three categories: what_good : Any variation on the question \"What was good about the service?\", or \"What did we do well?\" could_improve : Any variation on the question \"Please tell us about anything that we could have done better\", or \"How could we improve?\" nonspecific : Any other type of nonspecific question, e.g. \"Please can you tell us why you gave your answer?\", or \"What were you satisfied and/or dissatisfied with?\". # In Python text_data = [ { 'comment_id': '1', # The comment_id values in each dict must be unique. 'comment_text': 'This is the first comment. Nurse was great.', 'question_type': 'what_good' }, { 'comment_id': '2', 'comment_text': 'This is the second comment. The ward was freezing.', 'question_type': 'could_improve' }, { 'comment_id': '3', 'comment_text': '', # This comment is an empty string. 'question_type': 'nonspecific' } ] # In R library(jsonlite) comment_id <- c(\"1\", \"2\", \"3\") comment_text <- c( \"This is the first comment. Nurse was great.\", \"This is the second comment. The ward was freezing.\", \"\" ) question_type <- c(\"what_good\", \"could_improve\", \"nonspecific\") df <- data.frame(comment_id, comment_text, question_type) text_data <- toJSON(df) 2. Send the JSON containing the text data in a POST request to the API. Ensure that you include your API key, which should be stored securely. The model(s) used to make predictions can be selected with the target param. The options for this param are: m : multilabel s : sentiment ms : both multilabel and sentiment. # In Python api_key = os.getenv('API_KEY') params_dict = {'code': api_key, 'target': 'ms'} url = os.getenv('API_URL') response = requests.post(url, params= params_dict, json = text_data) # In R library(httr) api_key <- Sys.getenv(\"API_KEY\") params_dict <- list(code = api_key, target = \"ms\") url <- Sys.getenv(\"API_URL\") response <- POST(url, query = params_dict, body = text_data, encode = \"json\") 3. If the POST request is successful, you will receive a response with a 202 code, and a URL to retrieve your results, called the results URL . For example: # In Python if response.status_code == 202: results_url = response.text print(f\"URL for results is {results_url}\") # In R if (http_status(response) == 202) { results_url <- content(response, as = \"text\") } print(results_url) 4. Use a GET request to check the results URL. If your predictions are not yet ready, you will receive a 202 response. If they are ready, you will receive a 200 response. What is happening behind the scenes? The API has received your data and has started up a secure Azure container instance with your data stored in blob storage. The Docker container will install the pxtextmining package and make predictions using your data. Starting up a fresh container instance can take up to 5 minutes, and predictions using the slow transformer models can some time, up to 5 further minutes per 1000 comments. Once the predictions are complete, it will delete your data and save the predictions in blob storage. Once you receive a 200 response, your results are available in JSON format. Please note that this will only be available once; once you have collected the data, it will be deleted due to security reasons and your results URL will no longer be valid. You can set up a loop to check if your results are ready every 5 minutes, as follows. # In Python while True: results_response = requests.get(results_url) if results_response.status_code == 200: final_labels = results_response.json() break else: print('Not ready! Trying again in 300 seconds...') time.sleep(300) print('Predicted labels':) print(final_labels) # In R while (TRUE) { results_response <- GET(results_url) if (results_response$status_code == 200) { final_labels <- fromJSON(content(results_response, \"text\")) break } else { cat(\"Not ready! Trying again in 300 seconds...\\n\") Sys.sleep(300) } } cat(\"Predicted labels:\\n\") print(final_labels)","title":"Slow API"},{"location":"reference/API/slow_API/#slow-api","text":"This API is slower but uses the best performing models. The transformer-based Distilbert model consumes a lot of hardware resource, and as such required a different approach. For predicting the multilabel categories, the API endpoint ensembles together Support Vector Classifier, Gradient Boosted Decision Trees (XGBoost), and Distilbert models. For predicting text sentiment , the API endpoint utilises a Distilbert model. The API URL endpoint is available on request. You will need an API key, please contact the project team to obtain one. The key should be passed as a code param with your API request.","title":"Slow API"},{"location":"reference/API/slow_API/#how-to-make-an-api-call","text":"1. Prepare the data in JSON format. In Python, this is a list containing as many dict s as there are comments to be predicted. Each dict has three compulsory keys: comment_id : Unique ID associated with the comment, in str format. Each Comment ID per API call must be unique. comment_text : Text to be classified, in str format. question_type : The type of question asked to elicit the comment text. Questions are different from trust to trust, but they all fall into one of three categories: what_good : Any variation on the question \"What was good about the service?\", or \"What did we do well?\" could_improve : Any variation on the question \"Please tell us about anything that we could have done better\", or \"How could we improve?\" nonspecific : Any other type of nonspecific question, e.g. \"Please can you tell us why you gave your answer?\", or \"What were you satisfied and/or dissatisfied with?\". # In Python text_data = [ { 'comment_id': '1', # The comment_id values in each dict must be unique. 'comment_text': 'This is the first comment. Nurse was great.', 'question_type': 'what_good' }, { 'comment_id': '2', 'comment_text': 'This is the second comment. The ward was freezing.', 'question_type': 'could_improve' }, { 'comment_id': '3', 'comment_text': '', # This comment is an empty string. 'question_type': 'nonspecific' } ] # In R library(jsonlite) comment_id <- c(\"1\", \"2\", \"3\") comment_text <- c( \"This is the first comment. Nurse was great.\", \"This is the second comment. The ward was freezing.\", \"\" ) question_type <- c(\"what_good\", \"could_improve\", \"nonspecific\") df <- data.frame(comment_id, comment_text, question_type) text_data <- toJSON(df) 2. Send the JSON containing the text data in a POST request to the API. Ensure that you include your API key, which should be stored securely. The model(s) used to make predictions can be selected with the target param. The options for this param are: m : multilabel s : sentiment ms : both multilabel and sentiment. # In Python api_key = os.getenv('API_KEY') params_dict = {'code': api_key, 'target': 'ms'} url = os.getenv('API_URL') response = requests.post(url, params= params_dict, json = text_data) # In R library(httr) api_key <- Sys.getenv(\"API_KEY\") params_dict <- list(code = api_key, target = \"ms\") url <- Sys.getenv(\"API_URL\") response <- POST(url, query = params_dict, body = text_data, encode = \"json\") 3. If the POST request is successful, you will receive a response with a 202 code, and a URL to retrieve your results, called the results URL . For example: # In Python if response.status_code == 202: results_url = response.text print(f\"URL for results is {results_url}\") # In R if (http_status(response) == 202) { results_url <- content(response, as = \"text\") } print(results_url) 4. Use a GET request to check the results URL. If your predictions are not yet ready, you will receive a 202 response. If they are ready, you will receive a 200 response. What is happening behind the scenes? The API has received your data and has started up a secure Azure container instance with your data stored in blob storage. The Docker container will install the pxtextmining package and make predictions using your data. Starting up a fresh container instance can take up to 5 minutes, and predictions using the slow transformer models can some time, up to 5 further minutes per 1000 comments. Once the predictions are complete, it will delete your data and save the predictions in blob storage. Once you receive a 200 response, your results are available in JSON format. Please note that this will only be available once; once you have collected the data, it will be deleted due to security reasons and your results URL will no longer be valid. You can set up a loop to check if your results are ready every 5 minutes, as follows. # In Python while True: results_response = requests.get(results_url) if results_response.status_code == 200: final_labels = results_response.json() break else: print('Not ready! Trying again in 300 seconds...') time.sleep(300) print('Predicted labels':) print(final_labels) # In R while (TRUE) { results_response <- GET(results_url) if (results_response$status_code == 200) { final_labels <- fromJSON(content(results_response, \"text\")) break } else { cat(\"Not ready! Trying again in 300 seconds...\\n\") Sys.sleep(300) } } cat(\"Predicted labels:\\n\") print(final_labels)","title":"How to make an API call"},{"location":"reference/Docker/docker_README/","text":"Using our Docker container This Docker container contains the pxtextmining machine learning models trained as part of the Patient Experience Qualitative Data Categorisation project . To use this Docker container to predict your unlabelled text: 1. Set up your folders. You will need to set up a folder containing two other folders, data_in and data_out, as below. docker_data/ \u251c\u2500 data_in/ \u251c\u2500 data_out/ 2. Prepare your data. Save the data you wish to pass through the machine learning models as json, in the data_in folder. The data should be in the following format: In Python, a list containing as many dict s as there are comments to be predicted. Each dict has three compulsory keys: comment_id : Unique ID associated with the comment, in str format. Each Comment ID per API call must be unique. comment_text : Text to be classified, in str format. question_type : The type of question asked to elicit the comment text. Questions are different from trust to trust, but they all fall into one of three categories: what_good : Any variation on the question \"What was good about the service?\", or \"What did we do well?\" could_improve : Any variation on the question \"Please tell us about anything that we could have done better\", or \"How could we improve?\" nonspecific : Any other type of nonspecific question, e.g. \"Please can you tell us why you gave your answer?\", or \"What were you satisfied and/or dissatisfied with?\". # In Python text_data = [ { 'comment_id': '1', # The comment_id values in each dict must be unique. 'comment_text': 'This is the first comment. Nurse was great.', 'question_type': 'what_good' }, { 'comment_id': '2', 'comment_text': 'This is the second comment. The ward was freezing.', 'question_type': 'could_improve' }, { 'comment_id': '3', 'comment_text': '', # This comment is an empty string. 'question_type': 'nonspecific' } ] # In R library(jsonlite) comment_id <- c(\"1\", \"2\", \"3\") comment_text <- c( \"This is the first comment. Nurse was great.\", \"This is the second comment. The ward was freezing.\", \"\" ) question_type <- c(\"what_good\", \"could_improve\", \"nonspecific\") df <- data.frame(comment_id, comment_text, question_type) text_data <- toJSON(df) 3. Save the JSON data in the data_in folder, as follows: # In Python json_data = json.dumps(text_data) with open(\"data_in/file_01.json\", \"w\") as outfile: outfile.write(json_data) # In R json_data <- toJSON(text_data, pretty = TRUE) write(json_data, file = \"data_in/file_01.json\") 4. Your file structure should now look like this: docker_data/ \u251c\u2500 data_in/ \u2502 \u251c\u2500 file_01.json \u251c\u2500 data_out/ 5. Mount the docker_data folder as the data volume for the Docker container and run the container. Pass the filename for the input JSON as the first argument. The following arguments are also available: --local-storage or -l flag for local storage (does not delete the files in data_in after completing predictions) --target or -t to select the machine learning models used. Options are m for multilabel, s for sentiment , or ms for both. Defaults to ms if nothing is selected. A sample command would be: docker run --rm -it -v /docker_data:/data ghcr.io/the-strategy-unit/pxtextmining:latest file_01.json -l 6. The predictions will be outputted as a json file in the data_out folder, with the same filename. After running successfully, the final folder structure should be: docker_data/ \u251c\u2500 data_in/ \u2502 \u251c\u2500 file_01.json \u251c\u2500 data_out/ \u251c\u2500 file_01.json","title":"Using our Docker container"},{"location":"reference/Docker/docker_README/#using-our-docker-container","text":"This Docker container contains the pxtextmining machine learning models trained as part of the Patient Experience Qualitative Data Categorisation project . To use this Docker container to predict your unlabelled text: 1. Set up your folders. You will need to set up a folder containing two other folders, data_in and data_out, as below. docker_data/ \u251c\u2500 data_in/ \u251c\u2500 data_out/ 2. Prepare your data. Save the data you wish to pass through the machine learning models as json, in the data_in folder. The data should be in the following format: In Python, a list containing as many dict s as there are comments to be predicted. Each dict has three compulsory keys: comment_id : Unique ID associated with the comment, in str format. Each Comment ID per API call must be unique. comment_text : Text to be classified, in str format. question_type : The type of question asked to elicit the comment text. Questions are different from trust to trust, but they all fall into one of three categories: what_good : Any variation on the question \"What was good about the service?\", or \"What did we do well?\" could_improve : Any variation on the question \"Please tell us about anything that we could have done better\", or \"How could we improve?\" nonspecific : Any other type of nonspecific question, e.g. \"Please can you tell us why you gave your answer?\", or \"What were you satisfied and/or dissatisfied with?\". # In Python text_data = [ { 'comment_id': '1', # The comment_id values in each dict must be unique. 'comment_text': 'This is the first comment. Nurse was great.', 'question_type': 'what_good' }, { 'comment_id': '2', 'comment_text': 'This is the second comment. The ward was freezing.', 'question_type': 'could_improve' }, { 'comment_id': '3', 'comment_text': '', # This comment is an empty string. 'question_type': 'nonspecific' } ] # In R library(jsonlite) comment_id <- c(\"1\", \"2\", \"3\") comment_text <- c( \"This is the first comment. Nurse was great.\", \"This is the second comment. The ward was freezing.\", \"\" ) question_type <- c(\"what_good\", \"could_improve\", \"nonspecific\") df <- data.frame(comment_id, comment_text, question_type) text_data <- toJSON(df) 3. Save the JSON data in the data_in folder, as follows: # In Python json_data = json.dumps(text_data) with open(\"data_in/file_01.json\", \"w\") as outfile: outfile.write(json_data) # In R json_data <- toJSON(text_data, pretty = TRUE) write(json_data, file = \"data_in/file_01.json\") 4. Your file structure should now look like this: docker_data/ \u251c\u2500 data_in/ \u2502 \u251c\u2500 file_01.json \u251c\u2500 data_out/ 5. Mount the docker_data folder as the data volume for the Docker container and run the container. Pass the filename for the input JSON as the first argument. The following arguments are also available: --local-storage or -l flag for local storage (does not delete the files in data_in after completing predictions) --target or -t to select the machine learning models used. Options are m for multilabel, s for sentiment , or ms for both. Defaults to ms if nothing is selected. A sample command would be: docker run --rm -it -v /docker_data:/data ghcr.io/the-strategy-unit/pxtextmining:latest file_01.json -l 6. The predictions will be outputted as a json file in the data_out folder, with the same filename. After running successfully, the final folder structure should be: docker_data/ \u251c\u2500 data_in/ \u2502 \u251c\u2500 file_01.json \u251c\u2500 data_out/ \u251c\u2500 file_01.json","title":"Using our Docker container"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/","text":"factory_data_load_and_split df = load_multilabel_data(dataset, target='major_categories') module-attribute merge_categories(df, new_cat, cats_to_merge) Merges categories together in a dataset. Assumes all categories are all in the right format, one hot encoded with int values. Parameters: df ( DataFrame ) \u2013 DataFrame with labelled data. new_cat ( str ) \u2013 Name for new column of merged data. cats_to_merge ( list ) \u2013 List containing columns to be merged. Returns: DataFrame \u2013 DataFrame with new columns bert_data_to_dataset(X, Y=None, max_length=150, model_name='distilbert-base-uncased', additional_features=False) This function converts a dataframe into a format that can be utilised by a transformer model. If Y is provided then it returns a TensorFlow dataset for training the model. If Y is not provided, then it returns a dict which can be used to make predictions by an already trained model. Parameters: X ( DataFrame ) \u2013 Data to be converted to text data. Text should be in column 'FFT answer', FFT question should be in column 'FFT_q_standardised'. Y ( DataFrame , default: None ) \u2013 One Hot Encoded targets. Defaults to None. max_length ( int , default: 150 ) \u2013 Maximum length of text to be encoded. Defaults to 150. model_name ( str , default: 'distilbert-base-uncased' ) \u2013 Type of transformer model. Defaults to 'distilbert-base-uncased'. additional_features ( bool , default: False ) \u2013 Whether additional features are to be included, currently this is only question type in 'FFT_q_standardised' column. Defaults to False. Returns: tf.data.Dataset OR dict \u2013 tf.data.Dataset if Y is provided, dict otherwise. load_multilabel_data(filename, target='major_categories') Function for loading the multilabel dataset, converting it from csv to pd.DataFrame. Conducts some basic preprocessing, including standardisation of the question types, calculation of text length, and drops rows with no labels. Depending on selected target , returned dataframe contains different columns. Parameters: filename ( str ) \u2013 Path to file containing multilabel data, in csv format target ( str , default: 'major_categories' ) \u2013 Options are 'minor_categories', 'major_categories', or 'sentiment'. Defaults to 'major_categories'. Returns: DataFrame \u2013 DataFrame containing the columns 'FFT categorical answer', 'FFT question', and 'FFT answer'. Also conducts some clean_empty_features(text_dataframe) Replaces all empty whitespaces in a dataframe with np.NaN. Parameters: text_dataframe ( DataFrame ) \u2013 DataFrame containing text data with labels. Returns: DataFrame \u2013 DataFrame with all empty whitespaces replaced with np.NaN onehot(df, col_to_onehot) Function to one-hot encode specified columns in a dataframe. Parameters: df ( DataFrame ) \u2013 DataFrame containing data to be one-hot encoded col_to_onehot ( list ) \u2013 List of column names to be one-hot encoded Returns: DataFrame \u2013 One-hot encoded data process_data(df, target, preprocess_text=True, additional_features=False) Utilises remove_punc_and_nums and clean_empty_features functions to clean the text data and drop any rows that are only whitespace after cleaning. Also fills one-hot encoded columns with 0s rather than NaNs so that Y target is not sparse. Parameters: df ( DataFrame ) \u2013 DataFrame containing text data, any additional features, and targets target ( list ) \u2013 List of column names of targets preprocess_text ( bool , default: True ) \u2013 Whether or not text is to be processed with remove_punc_and_nums. If utilising an sklearn model then should be True. If utilising transformer-based BERT model then should be set to False. Defaults to True. additional_features ( bool , default: False ) \u2013 Whether or not 'question type' feature should be included. Defaults to False. Returns: tuple \u2013 Tuple containing two pd.DataFrames. The first contains the X features (text, with or without question type depending on additional_features), the second contains the one-hot encoded Y targets process_and_split_data(df, target, preprocess_text=True, additional_features=False, random_state=42) Combines the process_multilabel_data and train_test_split functions into one function Parameters: df ( DataFrame ) \u2013 DataFrame containing text data, any additional features, and targets target ( list ) \u2013 List of column names of targets preprocess_text ( bool , default: True ) \u2013 Whether or not text is to be processed with remove_punc_and_nums. If utilising an sklearn model then should be True. If utilising transformer-based BERT model then should be set to False. Defaults to True. additional_features ( bool , default: False ) \u2013 Whether or not 'question type' feature should be included. Defaults to False. random_state ( int , default: 42 ) \u2013 Controls the shuffling applied to the data before applying the split. Enables reproducible output across multiple function calls. Defaults to 42. Returns: list \u2013 List containing train-test split of preprocessed X features and Y targets. remove_punc_and_nums(text) Function to conduct basic preprocessing of text, removing punctuation and numbers, converting all text to lowercase, removing trailing whitespace. Parameters: text ( str ) \u2013 Str containing the text to be cleaned Returns: str \u2013 Cleaned text, all lowercased with no punctuation, numbers or trailing whitespace.","title":"Factory data load and split"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split","text":"","title":"factory_data_load_and_split"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.df","text":"","title":"df"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.merge_categories","text":"Merges categories together in a dataset. Assumes all categories are all in the right format, one hot encoded with int values. Parameters: df ( DataFrame ) \u2013 DataFrame with labelled data. new_cat ( str ) \u2013 Name for new column of merged data. cats_to_merge ( list ) \u2013 List containing columns to be merged. Returns: DataFrame \u2013 DataFrame with new columns","title":"merge_categories"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.bert_data_to_dataset","text":"This function converts a dataframe into a format that can be utilised by a transformer model. If Y is provided then it returns a TensorFlow dataset for training the model. If Y is not provided, then it returns a dict which can be used to make predictions by an already trained model. Parameters: X ( DataFrame ) \u2013 Data to be converted to text data. Text should be in column 'FFT answer', FFT question should be in column 'FFT_q_standardised'. Y ( DataFrame , default: None ) \u2013 One Hot Encoded targets. Defaults to None. max_length ( int , default: 150 ) \u2013 Maximum length of text to be encoded. Defaults to 150. model_name ( str , default: 'distilbert-base-uncased' ) \u2013 Type of transformer model. Defaults to 'distilbert-base-uncased'. additional_features ( bool , default: False ) \u2013 Whether additional features are to be included, currently this is only question type in 'FFT_q_standardised' column. Defaults to False. Returns: tf.data.Dataset OR dict \u2013 tf.data.Dataset if Y is provided, dict otherwise.","title":"bert_data_to_dataset"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.load_multilabel_data","text":"Function for loading the multilabel dataset, converting it from csv to pd.DataFrame. Conducts some basic preprocessing, including standardisation of the question types, calculation of text length, and drops rows with no labels. Depending on selected target , returned dataframe contains different columns. Parameters: filename ( str ) \u2013 Path to file containing multilabel data, in csv format target ( str , default: 'major_categories' ) \u2013 Options are 'minor_categories', 'major_categories', or 'sentiment'. Defaults to 'major_categories'. Returns: DataFrame \u2013 DataFrame containing the columns 'FFT categorical answer', 'FFT question', and 'FFT answer'. Also conducts some","title":"load_multilabel_data"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.clean_empty_features","text":"Replaces all empty whitespaces in a dataframe with np.NaN. Parameters: text_dataframe ( DataFrame ) \u2013 DataFrame containing text data with labels. Returns: DataFrame \u2013 DataFrame with all empty whitespaces replaced with np.NaN","title":"clean_empty_features"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.onehot","text":"Function to one-hot encode specified columns in a dataframe. Parameters: df ( DataFrame ) \u2013 DataFrame containing data to be one-hot encoded col_to_onehot ( list ) \u2013 List of column names to be one-hot encoded Returns: DataFrame \u2013 One-hot encoded data","title":"onehot"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.process_data","text":"Utilises remove_punc_and_nums and clean_empty_features functions to clean the text data and drop any rows that are only whitespace after cleaning. Also fills one-hot encoded columns with 0s rather than NaNs so that Y target is not sparse. Parameters: df ( DataFrame ) \u2013 DataFrame containing text data, any additional features, and targets target ( list ) \u2013 List of column names of targets preprocess_text ( bool , default: True ) \u2013 Whether or not text is to be processed with remove_punc_and_nums. If utilising an sklearn model then should be True. If utilising transformer-based BERT model then should be set to False. Defaults to True. additional_features ( bool , default: False ) \u2013 Whether or not 'question type' feature should be included. Defaults to False. Returns: tuple \u2013 Tuple containing two pd.DataFrames. The first contains the X features (text, with or without question type depending on additional_features), the second contains the one-hot encoded Y targets","title":"process_data"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.process_and_split_data","text":"Combines the process_multilabel_data and train_test_split functions into one function Parameters: df ( DataFrame ) \u2013 DataFrame containing text data, any additional features, and targets target ( list ) \u2013 List of column names of targets preprocess_text ( bool , default: True ) \u2013 Whether or not text is to be processed with remove_punc_and_nums. If utilising an sklearn model then should be True. If utilising transformer-based BERT model then should be set to False. Defaults to True. additional_features ( bool , default: False ) \u2013 Whether or not 'question type' feature should be included. Defaults to False. random_state ( int , default: 42 ) \u2013 Controls the shuffling applied to the data before applying the split. Enables reproducible output across multiple function calls. Defaults to 42. Returns: list \u2013 List containing train-test split of preprocessed X features and Y targets.","title":"process_and_split_data"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.remove_punc_and_nums","text":"Function to conduct basic preprocessing of text, removing punctuation and numbers, converting all text to lowercase, removing trailing whitespace. Parameters: text ( str ) \u2013 Str containing the text to be cleaned Returns: str \u2013 Cleaned text, all lowercased with no punctuation, numbers or trailing whitespace.","title":"remove_punc_and_nums"},{"location":"reference/pxtextmining/factories/factory_model_performance/","text":"factory_model_performance get_dummy_model(x_train, y_train) Creates dummy model that randomly predicts labels, fitted on the training data. Parameters: x_train ( DataFrame ) \u2013 Input features. y_train ( DataFrame ) \u2013 Target values. Returns: DummyClassifier \u2013 Trained dummy classifier. get_multiclass_metrics(x_test, y_test, labels, random_state, model, additional_features, training_time=None) Creates a string detailing various performance metrics for a multiclass model, which can then be written to a text file. Parameters: x_test ( DataFrame ) \u2013 DataFrame containing test dataset features y_test ( DataFrame ) \u2013 DataFrame containing test dataset true target values labels ( list ) \u2013 List containing the target labels random_state ( int ) \u2013 Seed used to control the shuffling of the data, to enable reproducible results. model ( tf.keras or sklearn model ) \u2013 Trained estimator. additional_features ( bool ) \u2013 Whether or not additional features (e.g. question type) have been included in training the model. Defaults to False. training_time ( str , default: None ) \u2013 Amount of time taken for model to train. Defaults to None. Raises: ValueError \u2013 Only models built with sklearn or tensorflow are allowed. Returns: str \u2013 String containing the model architecture/hyperparameters, random state used for the train test split, and classification report. get_multilabel_metrics(preds_df, y_test, labels, random_state, model, training_time=None) Creates a string detailing various performance metrics for a multilabel model, which can then be written to a text file. Parameters: preds_df ( DataFrame ) \u2013 DataFrame containing model predictions y_test ( DataFrame ) \u2013 DataFrame containing test dataset true target values labels ( list ) \u2013 List containing the target labels random_state ( int ) \u2013 Seed used to control the shuffling of the data, to enable reproducible results. model ( tf.keras or sklearn model ) \u2013 Trained estimator. training_time ( str , default: None ) \u2013 Amount of time taken for model to train. Defaults to None. Raises: ValueError \u2013 Only sklearn and tensorflow keras models allowed. Returns: str \u2013 String containing the model architecture/hyperparameters, random state used for the train test split, and performance metrics including: exact accuracy, hamming loss, macro jaccard score, and classification report. get_accuracy_per_class(y_test, pred) Function to produce accuracy per class for the predicted categories, compared against real values. Parameters: y_test ( Series ) \u2013 Test data (real target values). pred ( Series ) \u2013 Predicted target values. Returns: DataFrame \u2013 The computed accuracy per class metrics for the model. parse_metrics_file(metrics_file, labels) Reads performance metrics files that are written by factory_write_results.write_multilabel_models_and_metrics . Creates a pd.DataFrame with the precision, recall, f1_score, and support for each label, which can be filtered and sorted more easily. Parameters: metrics_file ( str ) \u2013 Path to the metrics file to be parsed. labels ( list ) \u2013 List of the target labels used in the metrics file. Returns: DataFrame \u2013 DataFrame containing the precision, recall, f1_score, and support for each label, as detailed in the performance metrics file. get_y_score(probs) Converts probabilities into format (n_samples, n_classes) so they can be passed into sklearn roc_auc_score function Parameters: probs ( ndarray ) \u2013 Probability estimates outputted by model Returns: ndarray \u2013 Probability estimates in format (n_samples, n_classes) additional_analysis(preds_df, y_true, labels, custom_threshold_dict=None) For given predictions, returns dataframe containing: macro one-vs-one ROC AUC score, number of True Positives, True Negatives, False Positives, and False Negatives. Parameters: preds_df ( DataFrame ) \u2013 Dataframe containing predicted labels in one-hot encoded format y_true ( array ) \u2013 One-hot encoded real Y values labels ( List ) \u2013 List of the target labels Returns: DataFrame \u2013 dataframe containing: macro one-vs-one ROC AUC score, number of True Positives, True Negatives, False Positives, and False Negatives.","title":"Factory model performance"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance","text":"","title":"factory_model_performance"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance.get_dummy_model","text":"Creates dummy model that randomly predicts labels, fitted on the training data. Parameters: x_train ( DataFrame ) \u2013 Input features. y_train ( DataFrame ) \u2013 Target values. Returns: DummyClassifier \u2013 Trained dummy classifier.","title":"get_dummy_model"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance.get_multiclass_metrics","text":"Creates a string detailing various performance metrics for a multiclass model, which can then be written to a text file. Parameters: x_test ( DataFrame ) \u2013 DataFrame containing test dataset features y_test ( DataFrame ) \u2013 DataFrame containing test dataset true target values labels ( list ) \u2013 List containing the target labels random_state ( int ) \u2013 Seed used to control the shuffling of the data, to enable reproducible results. model ( tf.keras or sklearn model ) \u2013 Trained estimator. additional_features ( bool ) \u2013 Whether or not additional features (e.g. question type) have been included in training the model. Defaults to False. training_time ( str , default: None ) \u2013 Amount of time taken for model to train. Defaults to None. Raises: ValueError \u2013 Only models built with sklearn or tensorflow are allowed. Returns: str \u2013 String containing the model architecture/hyperparameters, random state used for the train test split, and classification report.","title":"get_multiclass_metrics"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance.get_multilabel_metrics","text":"Creates a string detailing various performance metrics for a multilabel model, which can then be written to a text file. Parameters: preds_df ( DataFrame ) \u2013 DataFrame containing model predictions y_test ( DataFrame ) \u2013 DataFrame containing test dataset true target values labels ( list ) \u2013 List containing the target labels random_state ( int ) \u2013 Seed used to control the shuffling of the data, to enable reproducible results. model ( tf.keras or sklearn model ) \u2013 Trained estimator. training_time ( str , default: None ) \u2013 Amount of time taken for model to train. Defaults to None. Raises: ValueError \u2013 Only sklearn and tensorflow keras models allowed. Returns: str \u2013 String containing the model architecture/hyperparameters, random state used for the train test split, and performance metrics including: exact accuracy, hamming loss, macro jaccard score, and classification report.","title":"get_multilabel_metrics"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance.get_accuracy_per_class","text":"Function to produce accuracy per class for the predicted categories, compared against real values. Parameters: y_test ( Series ) \u2013 Test data (real target values). pred ( Series ) \u2013 Predicted target values. Returns: DataFrame \u2013 The computed accuracy per class metrics for the model.","title":"get_accuracy_per_class"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance.parse_metrics_file","text":"Reads performance metrics files that are written by factory_write_results.write_multilabel_models_and_metrics . Creates a pd.DataFrame with the precision, recall, f1_score, and support for each label, which can be filtered and sorted more easily. Parameters: metrics_file ( str ) \u2013 Path to the metrics file to be parsed. labels ( list ) \u2013 List of the target labels used in the metrics file. Returns: DataFrame \u2013 DataFrame containing the precision, recall, f1_score, and support for each label, as detailed in the performance metrics file.","title":"parse_metrics_file"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance.get_y_score","text":"Converts probabilities into format (n_samples, n_classes) so they can be passed into sklearn roc_auc_score function Parameters: probs ( ndarray ) \u2013 Probability estimates outputted by model Returns: ndarray \u2013 Probability estimates in format (n_samples, n_classes)","title":"get_y_score"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance.additional_analysis","text":"For given predictions, returns dataframe containing: macro one-vs-one ROC AUC score, number of True Positives, True Negatives, False Positives, and False Negatives. Parameters: preds_df ( DataFrame ) \u2013 Dataframe containing predicted labels in one-hot encoded format y_true ( array ) \u2013 One-hot encoded real Y values labels ( List ) \u2013 List of the target labels Returns: DataFrame \u2013 dataframe containing: macro one-vs-one ROC AUC score, number of True Positives, True Negatives, False Positives, and False Negatives.","title":"additional_analysis"},{"location":"reference/pxtextmining/factories/factory_pipeline/","text":"factory_pipeline model_name = model_name module-attribute create_sklearn_pipeline_sentiment(model_type, num_classes, additional_features=False) Creates sklearn pipeline and hyperparameter grid for searching, for a multiclass target. Parameters: model_type ( str ) \u2013 Allows for selection of different estimators. Permitted values are \"svm\" (Support Vector Classifier), or \"xgb\" (XGBoost). num_classes ( int ) \u2013 Number of target classes. additional_features ( bool , default: False ) \u2013 Whether or not additional features (question type, text length) are to be included in the features fed into the model. Defaults to True. Returns: tuple \u2013 Tuple containing the sklearn.pipeline.Pipeline with the selected estimator, and a dict containing the hyperparameters to be tuned. create_bert_model(Y_train, model_name=model_name, max_length=150, multilabel=True) Creates Transformer based model trained on text data, with last layer added on for multilabel classification task. Number of neurons in last layer depends on number of labels in Y target. Parameters: Y_train ( DataFrame ) \u2013 DataFrame containing one-hot encoded targets model_name ( str , default: model_name ) \u2013 Type of pretrained transformer model to load. Defaults to model_name set in pxtextmining.params max_length ( int , default: 150 ) \u2013 Maximum length of text to be passed through transformer model. Defaults to 150. multilabel ( Bool , default: True ) \u2013 Whether the target is multilabel or not. If set to False, target is multiclass. Defaults to True. Returns: Model \u2013 Compiled Tensforflow Keras model with pretrained transformer layers and last layer suited for multilabel classification task create_bert_model_additional_features(Y_train, model_name=model_name, max_length=150, multilabel=True) Creates Transformer based model trained on text data, concatenated with an additional Dense layer taking additional inputs, with last layer added on for multilabel classification task. Number of neurons in last layer depends on number of labels in Y target. Parameters: Y_train ( DataFrame ) \u2013 DataFrame containing one-hot encoded targets model_name ( str , default: model_name ) \u2013 Type of pretrained transformer model to load. Defaults to model_name set in pxtextmining.params max_length ( int , default: 150 ) \u2013 Maximum length of text to be passed through transformer model. Defaults to 150. multilabel ( Bool , default: True ) \u2013 Whether the target is multilabel or not. If set to False, target is multiclass. Defaults to True. Returns: Model \u2013 Compiled Tensforflow Keras model with pretrained transformer layers, three question_type inputs passed through a Dense layer, and last layer suited for multilabel classification task train_bert_model(train_dataset, val_dataset, model, class_weights_dict=None, epochs=30) Trains compiled transformer model with early stopping. Parameters: train_dataset ( Dataset ) \u2013 Train dataset, tokenized with huggingface tokenizer, in tf.data.Dataset format val_dataset ( Dataset ) \u2013 Validation dataset, tokenized with huggingface tokenizer, in tf.data.Dataset format model ( Model ) \u2013 Compiled transformer model with additional layers for specific task. class_weights_dict ( dict , default: None ) \u2013 Dict containing class weights for each target class. Defaults to None. epochs ( int , default: 30 ) \u2013 Number of epochs to train model for. Defaults to 30. Returns: tuple \u2013 Tuple containing trained model and the training time as a str. calculating_class_weights(y_true) Function for calculating class weights for target classes, to be used when fitting a model. Parameters: y_true ( DataFrame ) \u2013 Dataset containing onehot encoded multilabel targets Returns: dict \u2013 Dict containing calculated class weights for each target label. create_sklearn_vectorizer() Creates vectorizer for use with sklearn models, using sklearn tokenizer Returns: TfidfVectorizer \u2013 sklearn TfidfVectorizer create_sklearn_pipeline(model_type, additional_features=True) Creates sklearn pipeline and hyperparameter grid for searching, depending on model_type selected. Parameters: model_type ( str ) \u2013 Allows for selection of different estimators. Permitted values are \"mnb\" (Multinomial Naive Bayes), \"knn\" (K Nearest Neighbours), \"svm\" (Support Vector Classifier), or \"rfc\" (Random Forest Classifier). additional_features ( bool , default: True ) \u2013 Whether or not additional features (question type, text length) are to be included in the features fed into the model. Defaults to True. Returns: tuple \u2013 Tuple containing the sklearn.pipeline.Pipeline with the selected estimator, and a dict containing the hyperparameters to be tuned. search_sklearn_pipelines(X_train, Y_train, models_to_try, target=None, additional_features=True) Iterates through selected estimators, instantiating the relevant sklearn pipelines and searching for the optimum hyperparameters. Parameters: X_train ( DataFrame ) \u2013 DataFrame containing the features to be fed into the estimator Y_train ( DataFrame ) \u2013 DataFrame containing the targets models_to_try ( list ) \u2013 List containing the estimator types to be tried. Permitted values are \"mnb\" (Multinomial Naive Bayes), \"knn\" (K Nearest Neighbours), \"svm\" (Support Vector Classifier), or \"rfc\" (Random Forest Classifier). additional_features ( bool , default: True ) \u2013 Whether or not additional features (question type, text length) are to be included in the features fed into the model. Defaults to True. Raises: ValueError \u2013 If model_type includes value other than permitted values \"mnb\", \"knn\", \"svm\", or \"rfc\" Returns: tuple \u2013 Tuple containing: a list containing the refitted pipelines with the best hyperparameters identified in the search, and a list containing the training times for each of the pipelines. create_and_train_svc_model(X_train, Y_train, additional_features=False) Creates pipeline with a Support Vector Classifier using specific hyperparameters identified through previous gridsearching. Parameters: X_train ( DataFrame ) \u2013 DataFrame containing the features to be fed into the estimator Y_train ( DataFrame ) \u2013 DataFrame containing the targets Returns: tuple \u2013 Tuple containing: a fitted pipeline with a MultiOutputClassifier utilising a Support Vector Classifier estimator, and a str of the training time taken for the fitting of the pipeline.","title":"Factory pipeline"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline","text":"","title":"factory_pipeline"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.model_name","text":"","title":"model_name"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_sklearn_pipeline_sentiment","text":"Creates sklearn pipeline and hyperparameter grid for searching, for a multiclass target. Parameters: model_type ( str ) \u2013 Allows for selection of different estimators. Permitted values are \"svm\" (Support Vector Classifier), or \"xgb\" (XGBoost). num_classes ( int ) \u2013 Number of target classes. additional_features ( bool , default: False ) \u2013 Whether or not additional features (question type, text length) are to be included in the features fed into the model. Defaults to True. Returns: tuple \u2013 Tuple containing the sklearn.pipeline.Pipeline with the selected estimator, and a dict containing the hyperparameters to be tuned.","title":"create_sklearn_pipeline_sentiment"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_bert_model","text":"Creates Transformer based model trained on text data, with last layer added on for multilabel classification task. Number of neurons in last layer depends on number of labels in Y target. Parameters: Y_train ( DataFrame ) \u2013 DataFrame containing one-hot encoded targets model_name ( str , default: model_name ) \u2013 Type of pretrained transformer model to load. Defaults to model_name set in pxtextmining.params max_length ( int , default: 150 ) \u2013 Maximum length of text to be passed through transformer model. Defaults to 150. multilabel ( Bool , default: True ) \u2013 Whether the target is multilabel or not. If set to False, target is multiclass. Defaults to True. Returns: Model \u2013 Compiled Tensforflow Keras model with pretrained transformer layers and last layer suited for multilabel classification task","title":"create_bert_model"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_bert_model_additional_features","text":"Creates Transformer based model trained on text data, concatenated with an additional Dense layer taking additional inputs, with last layer added on for multilabel classification task. Number of neurons in last layer depends on number of labels in Y target. Parameters: Y_train ( DataFrame ) \u2013 DataFrame containing one-hot encoded targets model_name ( str , default: model_name ) \u2013 Type of pretrained transformer model to load. Defaults to model_name set in pxtextmining.params max_length ( int , default: 150 ) \u2013 Maximum length of text to be passed through transformer model. Defaults to 150. multilabel ( Bool , default: True ) \u2013 Whether the target is multilabel or not. If set to False, target is multiclass. Defaults to True. Returns: Model \u2013 Compiled Tensforflow Keras model with pretrained transformer layers, three question_type inputs passed through a Dense layer, and last layer suited for multilabel classification task","title":"create_bert_model_additional_features"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.train_bert_model","text":"Trains compiled transformer model with early stopping. Parameters: train_dataset ( Dataset ) \u2013 Train dataset, tokenized with huggingface tokenizer, in tf.data.Dataset format val_dataset ( Dataset ) \u2013 Validation dataset, tokenized with huggingface tokenizer, in tf.data.Dataset format model ( Model ) \u2013 Compiled transformer model with additional layers for specific task. class_weights_dict ( dict , default: None ) \u2013 Dict containing class weights for each target class. Defaults to None. epochs ( int , default: 30 ) \u2013 Number of epochs to train model for. Defaults to 30. Returns: tuple \u2013 Tuple containing trained model and the training time as a str.","title":"train_bert_model"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.calculating_class_weights","text":"Function for calculating class weights for target classes, to be used when fitting a model. Parameters: y_true ( DataFrame ) \u2013 Dataset containing onehot encoded multilabel targets Returns: dict \u2013 Dict containing calculated class weights for each target label.","title":"calculating_class_weights"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_sklearn_vectorizer","text":"Creates vectorizer for use with sklearn models, using sklearn tokenizer Returns: TfidfVectorizer \u2013 sklearn TfidfVectorizer","title":"create_sklearn_vectorizer"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_sklearn_pipeline","text":"Creates sklearn pipeline and hyperparameter grid for searching, depending on model_type selected. Parameters: model_type ( str ) \u2013 Allows for selection of different estimators. Permitted values are \"mnb\" (Multinomial Naive Bayes), \"knn\" (K Nearest Neighbours), \"svm\" (Support Vector Classifier), or \"rfc\" (Random Forest Classifier). additional_features ( bool , default: True ) \u2013 Whether or not additional features (question type, text length) are to be included in the features fed into the model. Defaults to True. Returns: tuple \u2013 Tuple containing the sklearn.pipeline.Pipeline with the selected estimator, and a dict containing the hyperparameters to be tuned.","title":"create_sklearn_pipeline"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.search_sklearn_pipelines","text":"Iterates through selected estimators, instantiating the relevant sklearn pipelines and searching for the optimum hyperparameters. Parameters: X_train ( DataFrame ) \u2013 DataFrame containing the features to be fed into the estimator Y_train ( DataFrame ) \u2013 DataFrame containing the targets models_to_try ( list ) \u2013 List containing the estimator types to be tried. Permitted values are \"mnb\" (Multinomial Naive Bayes), \"knn\" (K Nearest Neighbours), \"svm\" (Support Vector Classifier), or \"rfc\" (Random Forest Classifier). additional_features ( bool , default: True ) \u2013 Whether or not additional features (question type, text length) are to be included in the features fed into the model. Defaults to True. Raises: ValueError \u2013 If model_type includes value other than permitted values \"mnb\", \"knn\", \"svm\", or \"rfc\" Returns: tuple \u2013 Tuple containing: a list containing the refitted pipelines with the best hyperparameters identified in the search, and a list containing the training times for each of the pipelines.","title":"search_sklearn_pipelines"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_and_train_svc_model","text":"Creates pipeline with a Support Vector Classifier using specific hyperparameters identified through previous gridsearching. Parameters: X_train ( DataFrame ) \u2013 DataFrame containing the features to be fed into the estimator Y_train ( DataFrame ) \u2013 DataFrame containing the targets Returns: tuple \u2013 Tuple containing: a fitted pipeline with a MultiOutputClassifier utilising a Support Vector Classifier estimator, and a str of the training time taken for the fitting of the pipeline.","title":"create_and_train_svc_model"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/","text":"factory_predict_unlabelled_text process_text(text) Enacts same text preprocessing as is found in factory_data_load_and_split when creating training data. Converts to string, removes trailing whitespaces, null values, punctuation and numbers. Converts to lowercase. Parameters: text ( Series ) \u2013 Series containing data to be cleaned. Returns: Series \u2013 Processed text data predict_multilabel_sklearn(data, model, labels=minor_cats, additional_features=False, label_fix=True, rules_dict=None, custom_threshold_dict=None) Conducts basic preprocessing to remove punctuation and numbers. Utilises a pretrained sklearn machine learning model to make multilabel predictions on the cleaned text. Also takes the class with the highest predicted probability as the predicted class in cases where no class has been predicted, if fix_no_labels = True. Parameters: data ( pd.Series OR pd.DataFrame ) \u2013 DataFrame or Series containing data to be processed and utilised for predictions. Must be DataFrame with columns 'FFT answer' and 'FFT_q_standardised' if additional_features = True model ( base ) \u2013 Trained sklearn estimator able to perform multilabel classification. labels ( list , default: minor_cats ) \u2013 List containing target labels. Defaults to major_cats. additional_features ( bool , default: False ) \u2013 Whether or not FFT_q_standardised is included in data. Defaults to False. label_fix ( bool , default: True ) \u2013 Whether or not the class with the highest probability is taken as the predicted class in cases where no classes are predicted. Defaults to True. rules_dict ( dict , default: None ) \u2013 If supplied, enhances predicted probabilities of specific classes with rules based on the text vocabulary. custom_threshold_dict ( dict , default: None ) \u2013 If supplied, binary predictions are based on custom thresholds for specific classes. If not specified, defaults to threshold of 0.5. Returns: DataFrame \u2013 DataFrame containing one hot encoded predictions, and a column with a list of the predicted labels. predict_multilabel_bert(data, model, labels=minor_cats, additional_features=False, label_fix=True, custom_threshold_dict=None, rules_dict=None) Conducts basic preprocessing to remove blank text. Utilises a pretrained transformer-based machine learning model to make multilabel predictions on the cleaned text. Also takes the class with the highest predicted probability as the predicted class in cases where no class has been predicted, if fix_no_labels = True. Parameters: data ( pd.Series, pd.DataFrame, or tf.data.Dataset ) \u2013 DataFrame, Series, or Tensorflow Dataset containing data to be processed and utilised for predictions. Must be DataFrame with columns 'FFT answer' and 'FFT_q_standardised' if additional_features = True model ( Model ) \u2013 Trained tensorflow estimator able to perform multilabel classification. labels ( list , default: minor_cats ) \u2013 List containing target labels. Defaults to major_cats. additional_features ( bool , default: False ) \u2013 Whether or not FFT_q_standardised is included in data. Defaults to False. label_fix ( bool , default: True ) \u2013 Whether or not the class with the highest probability is taken as the predicted class in cases where no classes are predicted. Defaults to True. custom_threshold_dict ( dict , default: None ) \u2013 If custom thresholds for each label probability should be used. If none provided, default of 0.5 is used where a label is given if the probability is > 0.5. Keys of dict should correspond to labels. rules_dict ( dict , default: None ) \u2013 If supplied, enhances predicted probabilities of specific classes with rules based on the text vocabulary. Returns: DataFrame \u2013 DataFrame containing one hot encoded predictions, and a column with a list of the predicted labels. predict_sentiment_bert(data, model, additional_features=False, preprocess_text=False) Conducts basic preprocessing to remove blank text. Utilises a pretrained transformer-based machine learning model to make multilabel predictions on the cleaned text. Also takes the class with the highest predicted probability as the predicted class in cases where no class has been predicted, if fix_no_labels = True. Parameters: data ( pd.Series OR pd.DataFrame ) \u2013 DataFrame or Series containing data to be processed and utilised for predictions. Must be DataFrame with columns 'FFT answer' and 'FFT_q_standardised' if additional_features = True model ( Model ) \u2013 Trained tensorflow estimator able to perform multilabel classification. additional_features ( bool , default: False ) \u2013 Whether or not FFT_q_standardised is included in data. Defaults to False. preprocess_text ( bool , default: False ) \u2013 Whether or not text is to be preprocessed (punctuation and numbers removed). Returns: DataFrame \u2013 DataFrame containing input data and predicted sentiment predict_multiclass_bert(x, model, additional_features) Makes multiclass predictions using a transformer-based model. Can encode the data if not already encoded. Parameters: x ( DataFrame ) \u2013 DataFrame containing features to be passed through model. model ( Model ) \u2013 Pretrained transformer based model in tensorflow keras. additional_features ( bool ) \u2013 Whether or not additional features (e.g. question type) are included. Defaults to False. Returns: array \u2013 Predicted labels in one-hot encoded format. predict_with_probs(x, model, labels) Given a trained model and some features, makes predictions based on the model's outputted probabilities using the model.predict_proba function. Any label with a predicted probability over 0.5 is taken as the predicted label. If no labels are over 0.5 probability then the label with the highest probability is taken. Converts into one-hot encoded format (similar to what model.predict would output). Currently only works with sklearn models. Parameters: x ( DataFrame ) \u2013 Features to be used to make the prediction. model ( base ) \u2013 Trained sklearn multilabel classifier. labels ( list ) \u2013 List of labels for the categories to be predicted. Returns: array \u2013 Predicted labels in one hot encoded format based on model probability estimates. get_probabilities(label_series, labels, predicted_probabilities) Given a pd.Series containing labels, the list of labels, and a model's outputted predicted_probabilities for each label, create a dictionary containing the label and the predicted probability of that label. Parameters: label_series ( Series ) \u2013 Series containing labels in the format ['label_one', 'label_two'] labels ( list ) \u2013 List of the label names predicted_probabilities ( array ) \u2013 Predicted probabilities for each label Returns: Series \u2013 Series, each line containing a dict with the predicted probabilities for each label. get_labels(row, labels) Given a one-hot encoded row of predictions from a dataframe, returns a list containing the actual predicted labels as a str . Parameters: row ( DataFrame ) \u2013 Row in a DataFrame containing one-hot encoded predicted labels. labels ( list ) \u2013 List containing all the target labels, which should also be columns in the dataframe. Returns: list \u2013 List of the labels that have been predicted for the given text. predict_with_bert(data, model, max_length=150, additional_features=False) Makes predictions using a transformer-based model. Can encode the data if not already encoded. Parameters: data ( DataFrame ) \u2013 DataFrame containing features to be passed through model. model ( Model ) \u2013 Pretrained transformer based model in tensorflow keras. max_length ( int , default: 150 ) \u2013 If encoding is required, maximum length of input text. Defaults to 150. additional_features ( bool , default: False ) \u2013 Whether or not additional features (e.g. question type) are included. Defaults to False. Returns: array \u2013 Predicted probabilities for each label. fix_no_labels(binary_preds, predicted_probs) Function that takes in the binary predicted labels for a particular input, and the predicted probabilities for all the labels classes. Where no labels have been predicted for a particular input, takes the label with the highest predicted probability as the predicted label. Parameters: binary_preds ( array ) \u2013 Predicted labels, in a one-hot encoded binary format. Some rows may not have any predicted labels. predicted_probs ( array ) \u2013 Predicted probability of each label. Returns: array \u2013 Predicted labels in one-hot encoded format, with all rows containing at least one predicted label. turn_probs_into_binary(predicted_probs, custom_threshold_dict=None) Takes predicted probabilities (floats between 0 and 1) and converts these to binary outcomes. Scope to finetune this in later iterations of the project depending on the label and whether recall/precision is prioritised for that label. Parameters: predicted_probs ( array ) \u2013 Array containing the predicted probabilities for each class. Shape of array should be (num_samples, num_classes). Predicted probabilities should range from 0 to 1. Returns: array \u2013 Array containing binary outcomes for each label. Shape should remain the same as input, but values will be either 0 or 1. rulebased_probs(text, pred_probs, labels, rules_dict=rules_dict) Uses the rules_dict in pxtextmining.params to boost the probabilities of specific classes, given the appearance of specific words. Parameters: text ( Series ) \u2013 Series containing the text pred_probs ( ndarray ) \u2013 Numpy array containing the outputted predicted probabilities for the text. Returns: ndarray \u2013 Numpy array with the modified predicted probabilities of the text. get_thresholds(y_true, y_probs, labels) Uses sklearn.metrics.precision_recall_curve to calculate the best threshold to use to maximise F1 score for each of the labels, on a binary one-vs-rest basis. If zero division error occurs, the threshold is set to 0.5 automatically. Parameters: y_true ( array ) \u2013 Array containing true one-hot encoded labels, of shape (num_samples, num_labels) y_probs ( array ) \u2013 Array containing predicted probabilities labels. Can be 2d or 3d depending on whether sklearn or tensorflow.keras output. labels ( list ) \u2013 List of labels in target class. Returns: dict \u2013 Dict with key value pairs (label, recommended threshold) for maximising the F1 score. combine_predictions(df_list, labels, method='probabilities') Combines outputted prediction dataframes from different models, using different methods. Parameters: df_list ( list ) \u2013 List of predictions in pd.DataFrame format, produced with either predict_multilabel_sklearn or predict_multilabel_bert labels ( list ) \u2013 List of labels in the prediction dataframes. method ( str , default: 'probabilities' ) \u2013 Method to use for combining the predictions. Defaults to \"probabilities\", which uses the average of predicted probabilities from all models. This results in higher precision, lower recall, and the prediction threshold is lowered to 0.3. Otherwise, takes all predicted classes from all models (high recall, low precision). Returns: DataFrame \u2013 New predictions.","title":"Factory predict unlabelled text"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text","text":"","title":"factory_predict_unlabelled_text"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.process_text","text":"Enacts same text preprocessing as is found in factory_data_load_and_split when creating training data. Converts to string, removes trailing whitespaces, null values, punctuation and numbers. Converts to lowercase. Parameters: text ( Series ) \u2013 Series containing data to be cleaned. Returns: Series \u2013 Processed text data","title":"process_text"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.predict_multilabel_sklearn","text":"Conducts basic preprocessing to remove punctuation and numbers. Utilises a pretrained sklearn machine learning model to make multilabel predictions on the cleaned text. Also takes the class with the highest predicted probability as the predicted class in cases where no class has been predicted, if fix_no_labels = True. Parameters: data ( pd.Series OR pd.DataFrame ) \u2013 DataFrame or Series containing data to be processed and utilised for predictions. Must be DataFrame with columns 'FFT answer' and 'FFT_q_standardised' if additional_features = True model ( base ) \u2013 Trained sklearn estimator able to perform multilabel classification. labels ( list , default: minor_cats ) \u2013 List containing target labels. Defaults to major_cats. additional_features ( bool , default: False ) \u2013 Whether or not FFT_q_standardised is included in data. Defaults to False. label_fix ( bool , default: True ) \u2013 Whether or not the class with the highest probability is taken as the predicted class in cases where no classes are predicted. Defaults to True. rules_dict ( dict , default: None ) \u2013 If supplied, enhances predicted probabilities of specific classes with rules based on the text vocabulary. custom_threshold_dict ( dict , default: None ) \u2013 If supplied, binary predictions are based on custom thresholds for specific classes. If not specified, defaults to threshold of 0.5. Returns: DataFrame \u2013 DataFrame containing one hot encoded predictions, and a column with a list of the predicted labels.","title":"predict_multilabel_sklearn"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.predict_multilabel_bert","text":"Conducts basic preprocessing to remove blank text. Utilises a pretrained transformer-based machine learning model to make multilabel predictions on the cleaned text. Also takes the class with the highest predicted probability as the predicted class in cases where no class has been predicted, if fix_no_labels = True. Parameters: data ( pd.Series, pd.DataFrame, or tf.data.Dataset ) \u2013 DataFrame, Series, or Tensorflow Dataset containing data to be processed and utilised for predictions. Must be DataFrame with columns 'FFT answer' and 'FFT_q_standardised' if additional_features = True model ( Model ) \u2013 Trained tensorflow estimator able to perform multilabel classification. labels ( list , default: minor_cats ) \u2013 List containing target labels. Defaults to major_cats. additional_features ( bool , default: False ) \u2013 Whether or not FFT_q_standardised is included in data. Defaults to False. label_fix ( bool , default: True ) \u2013 Whether or not the class with the highest probability is taken as the predicted class in cases where no classes are predicted. Defaults to True. custom_threshold_dict ( dict , default: None ) \u2013 If custom thresholds for each label probability should be used. If none provided, default of 0.5 is used where a label is given if the probability is > 0.5. Keys of dict should correspond to labels. rules_dict ( dict , default: None ) \u2013 If supplied, enhances predicted probabilities of specific classes with rules based on the text vocabulary. Returns: DataFrame \u2013 DataFrame containing one hot encoded predictions, and a column with a list of the predicted labels.","title":"predict_multilabel_bert"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.predict_sentiment_bert","text":"Conducts basic preprocessing to remove blank text. Utilises a pretrained transformer-based machine learning model to make multilabel predictions on the cleaned text. Also takes the class with the highest predicted probability as the predicted class in cases where no class has been predicted, if fix_no_labels = True. Parameters: data ( pd.Series OR pd.DataFrame ) \u2013 DataFrame or Series containing data to be processed and utilised for predictions. Must be DataFrame with columns 'FFT answer' and 'FFT_q_standardised' if additional_features = True model ( Model ) \u2013 Trained tensorflow estimator able to perform multilabel classification. additional_features ( bool , default: False ) \u2013 Whether or not FFT_q_standardised is included in data. Defaults to False. preprocess_text ( bool , default: False ) \u2013 Whether or not text is to be preprocessed (punctuation and numbers removed). Returns: DataFrame \u2013 DataFrame containing input data and predicted sentiment","title":"predict_sentiment_bert"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.predict_multiclass_bert","text":"Makes multiclass predictions using a transformer-based model. Can encode the data if not already encoded. Parameters: x ( DataFrame ) \u2013 DataFrame containing features to be passed through model. model ( Model ) \u2013 Pretrained transformer based model in tensorflow keras. additional_features ( bool ) \u2013 Whether or not additional features (e.g. question type) are included. Defaults to False. Returns: array \u2013 Predicted labels in one-hot encoded format.","title":"predict_multiclass_bert"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.predict_with_probs","text":"Given a trained model and some features, makes predictions based on the model's outputted probabilities using the model.predict_proba function. Any label with a predicted probability over 0.5 is taken as the predicted label. If no labels are over 0.5 probability then the label with the highest probability is taken. Converts into one-hot encoded format (similar to what model.predict would output). Currently only works with sklearn models. Parameters: x ( DataFrame ) \u2013 Features to be used to make the prediction. model ( base ) \u2013 Trained sklearn multilabel classifier. labels ( list ) \u2013 List of labels for the categories to be predicted. Returns: array \u2013 Predicted labels in one hot encoded format based on model probability estimates.","title":"predict_with_probs"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.get_probabilities","text":"Given a pd.Series containing labels, the list of labels, and a model's outputted predicted_probabilities for each label, create a dictionary containing the label and the predicted probability of that label. Parameters: label_series ( Series ) \u2013 Series containing labels in the format ['label_one', 'label_two'] labels ( list ) \u2013 List of the label names predicted_probabilities ( array ) \u2013 Predicted probabilities for each label Returns: Series \u2013 Series, each line containing a dict with the predicted probabilities for each label.","title":"get_probabilities"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.get_labels","text":"Given a one-hot encoded row of predictions from a dataframe, returns a list containing the actual predicted labels as a str . Parameters: row ( DataFrame ) \u2013 Row in a DataFrame containing one-hot encoded predicted labels. labels ( list ) \u2013 List containing all the target labels, which should also be columns in the dataframe. Returns: list \u2013 List of the labels that have been predicted for the given text.","title":"get_labels"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.predict_with_bert","text":"Makes predictions using a transformer-based model. Can encode the data if not already encoded. Parameters: data ( DataFrame ) \u2013 DataFrame containing features to be passed through model. model ( Model ) \u2013 Pretrained transformer based model in tensorflow keras. max_length ( int , default: 150 ) \u2013 If encoding is required, maximum length of input text. Defaults to 150. additional_features ( bool , default: False ) \u2013 Whether or not additional features (e.g. question type) are included. Defaults to False. Returns: array \u2013 Predicted probabilities for each label.","title":"predict_with_bert"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.fix_no_labels","text":"Function that takes in the binary predicted labels for a particular input, and the predicted probabilities for all the labels classes. Where no labels have been predicted for a particular input, takes the label with the highest predicted probability as the predicted label. Parameters: binary_preds ( array ) \u2013 Predicted labels, in a one-hot encoded binary format. Some rows may not have any predicted labels. predicted_probs ( array ) \u2013 Predicted probability of each label. Returns: array \u2013 Predicted labels in one-hot encoded format, with all rows containing at least one predicted label.","title":"fix_no_labels"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.turn_probs_into_binary","text":"Takes predicted probabilities (floats between 0 and 1) and converts these to binary outcomes. Scope to finetune this in later iterations of the project depending on the label and whether recall/precision is prioritised for that label. Parameters: predicted_probs ( array ) \u2013 Array containing the predicted probabilities for each class. Shape of array should be (num_samples, num_classes). Predicted probabilities should range from 0 to 1. Returns: array \u2013 Array containing binary outcomes for each label. Shape should remain the same as input, but values will be either 0 or 1.","title":"turn_probs_into_binary"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.rulebased_probs","text":"Uses the rules_dict in pxtextmining.params to boost the probabilities of specific classes, given the appearance of specific words. Parameters: text ( Series ) \u2013 Series containing the text pred_probs ( ndarray ) \u2013 Numpy array containing the outputted predicted probabilities for the text. Returns: ndarray \u2013 Numpy array with the modified predicted probabilities of the text.","title":"rulebased_probs"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.get_thresholds","text":"Uses sklearn.metrics.precision_recall_curve to calculate the best threshold to use to maximise F1 score for each of the labels, on a binary one-vs-rest basis. If zero division error occurs, the threshold is set to 0.5 automatically. Parameters: y_true ( array ) \u2013 Array containing true one-hot encoded labels, of shape (num_samples, num_labels) y_probs ( array ) \u2013 Array containing predicted probabilities labels. Can be 2d or 3d depending on whether sklearn or tensorflow.keras output. labels ( list ) \u2013 List of labels in target class. Returns: dict \u2013 Dict with key value pairs (label, recommended threshold) for maximising the F1 score.","title":"get_thresholds"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.combine_predictions","text":"Combines outputted prediction dataframes from different models, using different methods. Parameters: df_list ( list ) \u2013 List of predictions in pd.DataFrame format, produced with either predict_multilabel_sklearn or predict_multilabel_bert labels ( list ) \u2013 List of labels in the prediction dataframes. method ( str , default: 'probabilities' ) \u2013 Method to use for combining the predictions. Defaults to \"probabilities\", which uses the average of predicted probabilities from all models. This results in higher precision, lower recall, and the prediction threshold is lowered to 0.3. Otherwise, takes all predicted classes from all models (high recall, low precision). Returns: DataFrame \u2013 New predictions.","title":"combine_predictions"},{"location":"reference/pxtextmining/factories/factory_write_results/","text":"factory_write_results write_multilabel_models_and_metrics(models, model_metrics, path) Saves models and their associated performance metrics into a specified folder Parameters: models ( list ) \u2013 List containing the trained tf.keras or sklearn models to be saved. model_metrics ( list ) \u2013 List containing the model metrics in str format path ( str ) \u2013 Path where model is to be saved. write_model_preds(x, y_true, preds_df, labels, path='labels.xlsx', return_df=False) Writes an Excel file to enable easier analysis of model outputs using the test set. Columns of the Excel file are: comment_id, actual_labels, predicted_labels, actual_label_probs, and predicted_label_probs. Parameters: x ( pd.Series OR pd.DataFrame ) \u2013 Text data used for predictions y_true ( array ) \u2013 Onehot encoded targets preds_df ( DataFrame ) \u2013 DataFrame containing predictions, predicted probabilities, and labels. Should be produced by predict_multilabel_sklearn or predict_multilabel_bert labels ( list ) \u2013 List containing target labels. path ( str , default: 'labels.xlsx' ) \u2013 Filename and path for file to be saved. Defaults to \"labels.xlsx\". return_df ( bool , default: False ) \u2013 Whether or not the processed data should be returned as a DataFrame. Defaults to False. Returns: DataFrame \u2013 DataFrame containing comment_id, comment text, actual_labels, predicted_labels, actual_label_probs, and predicted_label_probs. write_model_analysis(model_name, labels, dataset, path, preds_df=None, y_true=None, custom_threshold_dict=None) Writes an Excel file with the performance metrics of each label, as well as the counts of samples for each label. Parameters: model_name ( str ) \u2013 Model name used in the performance metrics file labels ( list ) \u2013 List of labels for the categories to be predicted. dataset ( DataFrame ) \u2013 Original dataset before train test split path ( str ) \u2013 Filepath where model and performance metrics file are saved.","title":"Factory write results"},{"location":"reference/pxtextmining/factories/factory_write_results/#pxtextmining.factories.factory_write_results","text":"","title":"factory_write_results"},{"location":"reference/pxtextmining/factories/factory_write_results/#pxtextmining.factories.factory_write_results.write_multilabel_models_and_metrics","text":"Saves models and their associated performance metrics into a specified folder Parameters: models ( list ) \u2013 List containing the trained tf.keras or sklearn models to be saved. model_metrics ( list ) \u2013 List containing the model metrics in str format path ( str ) \u2013 Path where model is to be saved.","title":"write_multilabel_models_and_metrics"},{"location":"reference/pxtextmining/factories/factory_write_results/#pxtextmining.factories.factory_write_results.write_model_preds","text":"Writes an Excel file to enable easier analysis of model outputs using the test set. Columns of the Excel file are: comment_id, actual_labels, predicted_labels, actual_label_probs, and predicted_label_probs. Parameters: x ( pd.Series OR pd.DataFrame ) \u2013 Text data used for predictions y_true ( array ) \u2013 Onehot encoded targets preds_df ( DataFrame ) \u2013 DataFrame containing predictions, predicted probabilities, and labels. Should be produced by predict_multilabel_sklearn or predict_multilabel_bert labels ( list ) \u2013 List containing target labels. path ( str , default: 'labels.xlsx' ) \u2013 Filename and path for file to be saved. Defaults to \"labels.xlsx\". return_df ( bool , default: False ) \u2013 Whether or not the processed data should be returned as a DataFrame. Defaults to False. Returns: DataFrame \u2013 DataFrame containing comment_id, comment text, actual_labels, predicted_labels, actual_label_probs, and predicted_label_probs.","title":"write_model_preds"},{"location":"reference/pxtextmining/factories/factory_write_results/#pxtextmining.factories.factory_write_results.write_model_analysis","text":"Writes an Excel file with the performance metrics of each label, as well as the counts of samples for each label. Parameters: model_name ( str ) \u2013 Model name used in the performance metrics file labels ( list ) \u2013 List of labels for the categories to be predicted. dataset ( DataFrame ) \u2013 Original dataset before train test split path ( str ) \u2013 Filepath where model and performance metrics file are saved.","title":"write_model_analysis"},{"location":"reference/pxtextmining/helpers/text_preprocessor/","text":"text_preprocessor tf_preprocessing(X, max_sentence_length=150) Conducts preprocessing with tensorflow tokenizer which vectorizes text and standardizes length. Parameters: X ( Series ) \u2013 Series containing the text to be processed max_sentence_length ( int , default: 150 ) \u2013 Maximum number of words. Defaults to 150. Returns: tuple \u2013 Tuple containing np.array of padded, tokenized, vectorized texts, and int showing number of unique words in vocabulary. Source code in pxtextmining\\helpers\\text_preprocessor.py def tf_preprocessing(X, max_sentence_length = 150): \"\"\"Conducts preprocessing with tensorflow tokenizer which vectorizes text and standardizes length. Args: X (pd.Series): Series containing the text to be processed max_sentence_length (int, optional): Maximum number of words. Defaults to 150. Returns: (tuple): Tuple containing `np.array` of padded, tokenized, vectorized texts, and `int` showing number of unique words in vocabulary. \"\"\" tk = Tokenizer() tk.fit_on_texts(X) vocab_size = len(tk.word_index) print(f'There are {vocab_size} different words in your corpus') X_token = tk.texts_to_sequences(X) ### Pad the inputs X_pad = pad_sequences(X_token, dtype='float32', padding='post', maxlen = max_sentence_length) return X_pad, vocab_size","title":"Text preprocessor"},{"location":"reference/pxtextmining/helpers/text_preprocessor/#pxtextmining.helpers.text_preprocessor","text":"","title":"text_preprocessor"},{"location":"reference/pxtextmining/helpers/text_preprocessor/#pxtextmining.helpers.text_preprocessor.tf_preprocessing","text":"Conducts preprocessing with tensorflow tokenizer which vectorizes text and standardizes length. Parameters: X ( Series ) \u2013 Series containing the text to be processed max_sentence_length ( int , default: 150 ) \u2013 Maximum number of words. Defaults to 150. Returns: tuple \u2013 Tuple containing np.array of padded, tokenized, vectorized texts, and int showing number of unique words in vocabulary. Source code in pxtextmining\\helpers\\text_preprocessor.py def tf_preprocessing(X, max_sentence_length = 150): \"\"\"Conducts preprocessing with tensorflow tokenizer which vectorizes text and standardizes length. Args: X (pd.Series): Series containing the text to be processed max_sentence_length (int, optional): Maximum number of words. Defaults to 150. Returns: (tuple): Tuple containing `np.array` of padded, tokenized, vectorized texts, and `int` showing number of unique words in vocabulary. \"\"\" tk = Tokenizer() tk.fit_on_texts(X) vocab_size = len(tk.word_index) print(f'There are {vocab_size} different words in your corpus') X_token = tk.texts_to_sequences(X) ### Pad the inputs X_pad = pad_sequences(X_token, dtype='float32', padding='post', maxlen = max_sentence_length) return X_pad, vocab_size","title":"tf_preprocessing"},{"location":"reference/pxtextmining/pipelines/multilabel_pipeline/","text":"multilabel_pipeline run_sklearn_pipeline(additional_features=False, target=major_cats, models_to_try=('mnb', 'knn', 'svm', 'rfc'), path='test_multilabel', include_analysis=False, custom_threshold=False) Runs all the functions required to load multilabel data, preprocess it, and split it into training and test sets. Creates sklearn pipelines and hyperparameters to search, using specified estimators. For each estimator type selected, performs a randomized search across the hyperparameters to identify the parameters providing the best results on the holdout data within the randomized search. Evaluates the performance of the refitted estimator with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Parameters: additional_features ( bool , default: False ) \u2013 Whether or not additional features (question type and text length) are used. Defaults to False. target ( list , default: major_cats ) \u2013 The target labels, which should be columns in the dataset DataFrame. Defaults to major_cats. models_to_try ( list , default: ('mnb', 'knn', 'svm', 'rfc') ) \u2013 List of the estimators to try. Defaults to [\"mnb\", \"knn\", \"svm\", \"rfc\"]. Permitted values are \"mnb\" (Multinomial Naive Bayes), \"knn\" (K Nearest Neighbours), \"svm\" (Support Vector Classifier), or \"rfc\" (Random Forest Classifier). path ( str , default: 'test_multilabel' ) \u2013 Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. include_analysis ( bool , default: False ) \u2013 Whether or not additional Excel files showing model performance and predicted labels are generated. Defaults to False. custom_threshold ( bool , default: False ) \u2013 Whether or not a custom classification threshold maximising the F1 score is to be calculated. Defaults to False. run_svc_pipeline(additional_features=False, target=major_cats, path='test_multilabel', include_analysis=False, custom_threshold=False) Runs all the functions required to load multilabel data, preprocess it, and split it into training and test sets. Creates sklearn pipeline using a MultiOutputClassifier and Support Vector Classifier estimator, with specific hyperparameters. Fits the pipeline on the training data. Evaluates the performance of the refitted estimator with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder, together with optional further analysis in the form of Excel files. Parameters: additional_features ( bool , default: False ) \u2013 Whether or not additional features (question type and text length) are used. Defaults to False. target ( list , default: major_cats ) \u2013 The target labels, which should be columns in the dataset DataFrame. Defaults to major_cats. path ( str , default: 'test_multilabel' ) \u2013 Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. include_analysis ( bool , default: False ) \u2013 Whether or not to create Excel files including further analysis of the model's performance. Defaults to False. If True, writes two Excel files to the specified folder, one containing the labels and the performance metrics for each label, and one containing the predicted labels and the actual labels for the test set, with the model's probabilities for both. custom_threshold ( bool , default: False ) \u2013 Whether or not a custom classification threshold maximising the F1 score is to be calculated. Defaults to False. run_bert_pipeline(additional_features=False, path='test_multilabel/bert', target=major_cats, include_analysis=False, custom_threshold=False) Runs all the functions required to load multilabel data, preprocess it, and split it into training, test and validation sets. Creates tf.keras Transformer model with additional layers specific to the classification task, and trains it on the train set. Evaluates the performance of trained model with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Parameters: additional_features ( bool , default: False ) \u2013 Whether or not additional features (question type and text length) are used. Defaults to False. path ( str , default: 'test_multilabel/bert' ) \u2013 Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. target ( list , default: major_cats ) \u2013 The target labels, which should be columns in the dataset DataFrame. Defaults to major_cats. include_analysis ( bool , default: False ) \u2013 Whether or not to create Excel files including further analysis of the model's performance. Defaults to False. If True, writes two Excel files to the specified folder, one containing the labels and the performance metrics for each label, and one containing the predicted labels and the actual labels for the test set, with the model's probabilities for both. custom_threshold ( bool , default: False ) \u2013 Whether or not a custom classification threshold maximising the F1 score is to be calculated. Defaults to False.","title":"Multilabel pipeline"},{"location":"reference/pxtextmining/pipelines/multilabel_pipeline/#pxtextmining.pipelines.multilabel_pipeline","text":"","title":"multilabel_pipeline"},{"location":"reference/pxtextmining/pipelines/multilabel_pipeline/#pxtextmining.pipelines.multilabel_pipeline.run_sklearn_pipeline","text":"Runs all the functions required to load multilabel data, preprocess it, and split it into training and test sets. Creates sklearn pipelines and hyperparameters to search, using specified estimators. For each estimator type selected, performs a randomized search across the hyperparameters to identify the parameters providing the best results on the holdout data within the randomized search. Evaluates the performance of the refitted estimator with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Parameters: additional_features ( bool , default: False ) \u2013 Whether or not additional features (question type and text length) are used. Defaults to False. target ( list , default: major_cats ) \u2013 The target labels, which should be columns in the dataset DataFrame. Defaults to major_cats. models_to_try ( list , default: ('mnb', 'knn', 'svm', 'rfc') ) \u2013 List of the estimators to try. Defaults to [\"mnb\", \"knn\", \"svm\", \"rfc\"]. Permitted values are \"mnb\" (Multinomial Naive Bayes), \"knn\" (K Nearest Neighbours), \"svm\" (Support Vector Classifier), or \"rfc\" (Random Forest Classifier). path ( str , default: 'test_multilabel' ) \u2013 Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. include_analysis ( bool , default: False ) \u2013 Whether or not additional Excel files showing model performance and predicted labels are generated. Defaults to False. custom_threshold ( bool , default: False ) \u2013 Whether or not a custom classification threshold maximising the F1 score is to be calculated. Defaults to False.","title":"run_sklearn_pipeline"},{"location":"reference/pxtextmining/pipelines/multilabel_pipeline/#pxtextmining.pipelines.multilabel_pipeline.run_svc_pipeline","text":"Runs all the functions required to load multilabel data, preprocess it, and split it into training and test sets. Creates sklearn pipeline using a MultiOutputClassifier and Support Vector Classifier estimator, with specific hyperparameters. Fits the pipeline on the training data. Evaluates the performance of the refitted estimator with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder, together with optional further analysis in the form of Excel files. Parameters: additional_features ( bool , default: False ) \u2013 Whether or not additional features (question type and text length) are used. Defaults to False. target ( list , default: major_cats ) \u2013 The target labels, which should be columns in the dataset DataFrame. Defaults to major_cats. path ( str , default: 'test_multilabel' ) \u2013 Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. include_analysis ( bool , default: False ) \u2013 Whether or not to create Excel files including further analysis of the model's performance. Defaults to False. If True, writes two Excel files to the specified folder, one containing the labels and the performance metrics for each label, and one containing the predicted labels and the actual labels for the test set, with the model's probabilities for both. custom_threshold ( bool , default: False ) \u2013 Whether or not a custom classification threshold maximising the F1 score is to be calculated. Defaults to False.","title":"run_svc_pipeline"},{"location":"reference/pxtextmining/pipelines/multilabel_pipeline/#pxtextmining.pipelines.multilabel_pipeline.run_bert_pipeline","text":"Runs all the functions required to load multilabel data, preprocess it, and split it into training, test and validation sets. Creates tf.keras Transformer model with additional layers specific to the classification task, and trains it on the train set. Evaluates the performance of trained model with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Parameters: additional_features ( bool , default: False ) \u2013 Whether or not additional features (question type and text length) are used. Defaults to False. path ( str , default: 'test_multilabel/bert' ) \u2013 Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. target ( list , default: major_cats ) \u2013 The target labels, which should be columns in the dataset DataFrame. Defaults to major_cats. include_analysis ( bool , default: False ) \u2013 Whether or not to create Excel files including further analysis of the model's performance. Defaults to False. If True, writes two Excel files to the specified folder, one containing the labels and the performance metrics for each label, and one containing the predicted labels and the actual labels for the test set, with the model's probabilities for both. custom_threshold ( bool , default: False ) \u2013 Whether or not a custom classification threshold maximising the F1 score is to be calculated. Defaults to False.","title":"run_bert_pipeline"},{"location":"reference/pxtextmining/pipelines/sentiment_pipeline/","text":"sentiment_pipeline random_state = 75 module-attribute run_sentiment_pipeline(additional_features=False, models_to_try=('svm', 'xgb'), path='test_multilabel/sentiment') Runs all the functions required to load multiclass data, preprocess it, and split it into training, test and validation sets. Creates sklearn model and hyperparameter grid to search, and trains it on the train set. Evaluates the performance of trained model with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Parameters: additional_features ( bool , default: False ) \u2013 Whether or not additional features (question type and text length) are used. Defaults to False. models_to_try ( list , default: ('svm', 'xgb') ) \u2013 Which model types to try. Defaults to [\"svm\", \"xgb\"]. path ( str , default: 'test_multilabel/sentiment' ) \u2013 Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. run_sentiment_bert_pipeline(additional_features=True, path='test_multilabel/sentiment_bert') Runs all the functions required to load multiclass data, preprocess it, and split it into training, test and validation sets. Creates tf.keras Transformer model with additional layers specific to the classification task, and trains it on the train set. Evaluates the performance of trained model with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Parameters: additional_features ( bool , default: True ) \u2013 Whether or not additional features (question type and text length) are used. Defaults to False. path ( str , default: 'test_multilabel/sentiment_bert' ) \u2013 Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'.","title":"Sentiment pipeline"},{"location":"reference/pxtextmining/pipelines/sentiment_pipeline/#pxtextmining.pipelines.sentiment_pipeline","text":"","title":"sentiment_pipeline"},{"location":"reference/pxtextmining/pipelines/sentiment_pipeline/#pxtextmining.pipelines.sentiment_pipeline.random_state","text":"","title":"random_state"},{"location":"reference/pxtextmining/pipelines/sentiment_pipeline/#pxtextmining.pipelines.sentiment_pipeline.run_sentiment_pipeline","text":"Runs all the functions required to load multiclass data, preprocess it, and split it into training, test and validation sets. Creates sklearn model and hyperparameter grid to search, and trains it on the train set. Evaluates the performance of trained model with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Parameters: additional_features ( bool , default: False ) \u2013 Whether or not additional features (question type and text length) are used. Defaults to False. models_to_try ( list , default: ('svm', 'xgb') ) \u2013 Which model types to try. Defaults to [\"svm\", \"xgb\"]. path ( str , default: 'test_multilabel/sentiment' ) \u2013 Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'.","title":"run_sentiment_pipeline"},{"location":"reference/pxtextmining/pipelines/sentiment_pipeline/#pxtextmining.pipelines.sentiment_pipeline.run_sentiment_bert_pipeline","text":"Runs all the functions required to load multiclass data, preprocess it, and split it into training, test and validation sets. Creates tf.keras Transformer model with additional layers specific to the classification task, and trains it on the train set. Evaluates the performance of trained model with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Parameters: additional_features ( bool , default: True ) \u2013 Whether or not additional features (question type and text length) are used. Defaults to False. path ( str , default: 'test_multilabel/sentiment_bert' ) \u2013 Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'.","title":"run_sentiment_bert_pipeline"}]}