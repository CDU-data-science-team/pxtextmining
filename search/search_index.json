{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home This site contains the project documentation for the pxtextmining python package. This provides a technical overview of the package; for a non-technical overview and further information, visit the Patient Experience Qualitative Data Categorisation website . Table Of Contents The documentation is split into three separate sections: Project background Getting started, a simple approach to using the package: Installation How the package works Training a new model Making predictions with a trained model Code reference, a more technical overview of the functions and modules: Factories Helpers Pipelines","title":"Home"},{"location":"#home","text":"This site contains the project documentation for the pxtextmining python package. This provides a technical overview of the package; for a non-technical overview and further information, visit the Patient Experience Qualitative Data Categorisation website .","title":"Home"},{"location":"#table-of-contents","text":"The documentation is split into three separate sections: Project background Getting started, a simple approach to using the package: Installation How the package works Training a new model Making predictions with a trained model Code reference, a more technical overview of the functions and modules: Factories Helpers Pipelines","title":"Table Of Contents"},{"location":"about/","text":"Project background The pxtextmining package is part of the Patient Experience Qualitative Data Categorisation project . This project is is hosted by Nottinghamshire Healthcare NHS Foundation Trust's Clinical Development Unit Data Science Team, and funded by NHS England's Insight and Feedback Team. The primary objective of the pxtextmining element is to create a machine learning model capable of categorising the free text data obtained through the NHS England Friends and Family Test (FFT). It is a multilabel classification problem, with one or more categories applied to each patient feedback comment. In this way, we hope to support better use of qualitative patient experience feedback by NHS provider organisations. This package works together with the experiencesdashboard , a frontend coded in R/Shiny.","title":"Project background"},{"location":"about/#project-background","text":"The pxtextmining package is part of the Patient Experience Qualitative Data Categorisation project . This project is is hosted by Nottinghamshire Healthcare NHS Foundation Trust's Clinical Development Unit Data Science Team, and funded by NHS England's Insight and Feedback Team. The primary objective of the pxtextmining element is to create a machine learning model capable of categorising the free text data obtained through the NHS England Friends and Family Test (FFT). It is a multilabel classification problem, with one or more categories applied to each patient feedback comment. In this way, we hope to support better use of qualitative patient experience feedback by NHS provider organisations. This package works together with the experiencesdashboard , a frontend coded in R/Shiny.","title":"Project background"},{"location":"getting%20started/install/","text":"Installation You can install pxtextmining from either PyPI or GitHub . The recommended method is to clone the repository from GitHub, as this will also include the models and datasets. Option 1: Install from PyPI This option allows you to use the functions coded in pxtextmining. Install pxtextmining and its PyPI dependencies: pip install pxtextmining We also need to install the spaCy model used in pxtextmining.helpers.tokenization Note that the second model is pretty large, so the installation may take a while. python -m spacy download en_core_web_lg Option 2 (RECOMMENDED): Install from GitHub This option is recommended as it gives you access to the full datasets and already trained models. To begin with, clone the repository from github . It is also recommended to create a new virtual environment , using your chosen method of managing Python environments. The package uses poetry for dependency management. First, run pip install poetry . Then, run poetry install --with dev . We also need to install the spaCy model used in pxtextmining.helpers.tokenization Note that the second model is pretty large, so the installation may take a while. python -m spacy download en_core_web_lg","title":"Install"},{"location":"getting%20started/install/#installation","text":"You can install pxtextmining from either PyPI or GitHub . The recommended method is to clone the repository from GitHub, as this will also include the models and datasets.","title":"Installation"},{"location":"getting%20started/install/#option-1-install-from-pypi","text":"This option allows you to use the functions coded in pxtextmining. Install pxtextmining and its PyPI dependencies: pip install pxtextmining We also need to install the spaCy model used in pxtextmining.helpers.tokenization Note that the second model is pretty large, so the installation may take a while. python -m spacy download en_core_web_lg","title":"Option 1: Install from PyPI"},{"location":"getting%20started/install/#option-2-recommended-install-from-github","text":"This option is recommended as it gives you access to the full datasets and already trained models. To begin with, clone the repository from github . It is also recommended to create a new virtual environment , using your chosen method of managing Python environments. The package uses poetry for dependency management. First, run pip install poetry . Then, run poetry install --with dev . We also need to install the spaCy model used in pxtextmining.helpers.tokenization Note that the second model is pretty large, so the installation may take a while. python -m spacy download en_core_web_lg","title":"Option 2 (RECOMMENDED): Install from GitHub"},{"location":"getting%20started/package/","text":"Package structure pxtextmining The pxtextmining package is constructed using the following elements: pxtextmining.factories This module contains vast majority of the code in the package. There are five different stages, each corresponding to a different submodule. factory_data_load_and_split : Loading of multilabel data, preprocessing, and splitting into train/test/validation sets as appropriate. factory_pipeline : Construction and training of different models/estimators/algorithms using the sklearn , tensorflow.keras and transformers libraries. factory_model_performance : Evaluation of a trained model, comparing predicted targets with real target values, to produce performance metrics. The decision-making process behind the peformance metrics chosen can be seen on the project documentation website . The performance metrics for the current best models utilised in the API can be found in the current_best_multilabel folder in the main repository. factory_predict_unlabelled_text : Prepares unlabelled text (with or without additional features such as question type) in a format suitable for each model type, and passes this through the selected models, to produce predicted labels. pxtextmining.helpers This module contains some helper functions which are used in pxtextmining.factories . Some of this is legacy code, so this may just be moved into the factories submodule in future versions of the package. pxtextmining.pipelines All of the processes in pxtextmining.factories are pulled together in multilabel_pipeline , to create the complete end-to-end process of data processing, model creation, training, evaluation, and saving. There is also a pxtextmining.params file which is used to standardise specific variables that are used across the entire package. The aim of this is to reduce repetition across the package, for example when trying different targets or model types. API Separate from the pxtextmining package is the API, which can be found in the folder api . It is constructed using FastAPI and Uvicorn. The aim of the API is to make the trained machine learning models available publicly, so that predictions can be made on any text. The API is not currently publicly available and access is only for participating partner trusts. However, all the code and documentation is available on our github repository.","title":"Package structure"},{"location":"getting%20started/package/#package-structure","text":"","title":"Package structure"},{"location":"getting%20started/package/#pxtextmining","text":"The pxtextmining package is constructed using the following elements: pxtextmining.factories This module contains vast majority of the code in the package. There are five different stages, each corresponding to a different submodule. factory_data_load_and_split : Loading of multilabel data, preprocessing, and splitting into train/test/validation sets as appropriate. factory_pipeline : Construction and training of different models/estimators/algorithms using the sklearn , tensorflow.keras and transformers libraries. factory_model_performance : Evaluation of a trained model, comparing predicted targets with real target values, to produce performance metrics. The decision-making process behind the peformance metrics chosen can be seen on the project documentation website . The performance metrics for the current best models utilised in the API can be found in the current_best_multilabel folder in the main repository. factory_predict_unlabelled_text : Prepares unlabelled text (with or without additional features such as question type) in a format suitable for each model type, and passes this through the selected models, to produce predicted labels. pxtextmining.helpers This module contains some helper functions which are used in pxtextmining.factories . Some of this is legacy code, so this may just be moved into the factories submodule in future versions of the package. pxtextmining.pipelines All of the processes in pxtextmining.factories are pulled together in multilabel_pipeline , to create the complete end-to-end process of data processing, model creation, training, evaluation, and saving. There is also a pxtextmining.params file which is used to standardise specific variables that are used across the entire package. The aim of this is to reduce repetition across the package, for example when trying different targets or model types.","title":"pxtextmining"},{"location":"getting%20started/package/#api","text":"Separate from the pxtextmining package is the API, which can be found in the folder api . It is constructed using FastAPI and Uvicorn. The aim of the API is to make the trained machine learning models available publicly, so that predictions can be made on any text. The API is not currently publicly available and access is only for participating partner trusts. However, all the code and documentation is available on our github repository.","title":"API"},{"location":"getting%20started/training_new_model/","text":"Training a new model To train a new model to categorise patient feedback text, labelled data is required. Discussions are currently underway to enable the release of the data that the multilabel models in pxtextmining are trained on. This page breaks down the steps in the function pxtextmining.pipelines.run_sklearn_pipeline , which outputs trained sklearn models. This is a high-level explanation of the processes; for more detailed technical information please see the relevant code reference pages for each function. # Step 1: Generate a random_state which is used for the train_test_split. # This means that the pipeline and evaluation should be reproducible. random_state = random.randint(1,999) # Step 2: Load the data and isolate the target columns from the dataframe. df = load_multilabel_data(filename = 'datasets/hidden/multilabeldata_2.csv', target = 'major_categories') # Step 3: Conduct preprocessing: remove punctuation and numbers, clean whitespace and drop empty lines. # Split into train and test using the random_state above. X_train, X_test, Y_train, Y_test = process_and_split_data( df, target = target, random_state = random_state) # Step 4: Instantiate a pipeline and hyperparamter grid for each estimator to be tried. # Conduct a cross-validated randomized search to identify the hyperparameters # producing the best results on the validation set. # For each estimator, returns the pipeline with the best hyperparameters, # together with the time taken to search the pipeline. models, training_times = search_sklearn_pipelines(X_train, Y_train, models_to_try = models_to_try, additional_features = additional_features) # Step 5: Evaluate each pipeline using the test set, comparing predicted values with real values. # Performance metrics are recorded together with the time taken to search the pipeline. model_metrics = [] for i in range(len(models)): m = models[i] t = training_times[i] model_metrics.append(get_multilabel_metrics(X_test, Y_test, random_state = random_state, labels = target, model_type = 'sklearn', model = m, training_time = t)) # Step 6: Save the models and performance metrics to the path specified write_multilabel_models_and_metrics(models,model_metrics,path=path)","title":"Training a new model"},{"location":"getting%20started/training_new_model/#training-a-new-model","text":"To train a new model to categorise patient feedback text, labelled data is required. Discussions are currently underway to enable the release of the data that the multilabel models in pxtextmining are trained on. This page breaks down the steps in the function pxtextmining.pipelines.run_sklearn_pipeline , which outputs trained sklearn models. This is a high-level explanation of the processes; for more detailed technical information please see the relevant code reference pages for each function. # Step 1: Generate a random_state which is used for the train_test_split. # This means that the pipeline and evaluation should be reproducible. random_state = random.randint(1,999) # Step 2: Load the data and isolate the target columns from the dataframe. df = load_multilabel_data(filename = 'datasets/hidden/multilabeldata_2.csv', target = 'major_categories') # Step 3: Conduct preprocessing: remove punctuation and numbers, clean whitespace and drop empty lines. # Split into train and test using the random_state above. X_train, X_test, Y_train, Y_test = process_and_split_data( df, target = target, random_state = random_state) # Step 4: Instantiate a pipeline and hyperparamter grid for each estimator to be tried. # Conduct a cross-validated randomized search to identify the hyperparameters # producing the best results on the validation set. # For each estimator, returns the pipeline with the best hyperparameters, # together with the time taken to search the pipeline. models, training_times = search_sklearn_pipelines(X_train, Y_train, models_to_try = models_to_try, additional_features = additional_features) # Step 5: Evaluate each pipeline using the test set, comparing predicted values with real values. # Performance metrics are recorded together with the time taken to search the pipeline. model_metrics = [] for i in range(len(models)): m = models[i] t = training_times[i] model_metrics.append(get_multilabel_metrics(X_test, Y_test, random_state = random_state, labels = target, model_type = 'sklearn', model = m, training_time = t)) # Step 6: Save the models and performance metrics to the path specified write_multilabel_models_and_metrics(models,model_metrics,path=path)","title":"Training a new model"},{"location":"getting%20started/using_trained_model/","text":"Using a trained model The current_best_multilabel folder should contain a fully trained sklearn model in .sav format, as well as performance metrics for the model. The Transformer-based tensorflow.keras model is over 1GB and cannot be shared via GitHub. However, it will be made available via the API, which is forthcoming in a future release of this package. This page breaks down the steps in the function pxtextmining.pipelines.factory_predict_unlabelled_text.predict_multilabel_sklearn , which can make predictions using the sklearn model available via GitHub. This is a high-level explanation of the processes; for more detailed technical information please see the relevant code reference page. # Step 1: Conduct preprocessing on text: # Temove trailing whitespaces, NULL values, NaNs, and punctuation. Converts to lowercase. text_no_whitespace = text.replace(r\"^\\s*$\", np.nan, regex=True) text_no_nans = text_no_whitespace.dropna() text_cleaned = text_no_nans.astype(str).apply(remove_punc_and_nums) processed_text = text_cleaned.astype(str).apply(clean_empty_features) # Step 2: Make predictions with the trained model binary_preds = model.predict(processed_text) # Step 3: Get predicted probabilities for each label pred_probs = np.array(model.predict_proba(processed_text)) # Step 4: Some samples do not have any predicted labels. # For these, take the label with the highest predicted probability. predictions = fix_no_labels(binary_preds, pred_probs, model_type=\"sklearn\") # Step 5: Convert predictions to a dataframe. preds_df = pd.DataFrame(predictions, index=processed_text.index, columns=labels) preds_df[\"labels\"] = preds_df.apply(get_labels, args=(labels,), axis=1)","title":"Using a trained model"},{"location":"getting%20started/using_trained_model/#using-a-trained-model","text":"The current_best_multilabel folder should contain a fully trained sklearn model in .sav format, as well as performance metrics for the model. The Transformer-based tensorflow.keras model is over 1GB and cannot be shared via GitHub. However, it will be made available via the API, which is forthcoming in a future release of this package. This page breaks down the steps in the function pxtextmining.pipelines.factory_predict_unlabelled_text.predict_multilabel_sklearn , which can make predictions using the sklearn model available via GitHub. This is a high-level explanation of the processes; for more detailed technical information please see the relevant code reference page. # Step 1: Conduct preprocessing on text: # Temove trailing whitespaces, NULL values, NaNs, and punctuation. Converts to lowercase. text_no_whitespace = text.replace(r\"^\\s*$\", np.nan, regex=True) text_no_nans = text_no_whitespace.dropna() text_cleaned = text_no_nans.astype(str).apply(remove_punc_and_nums) processed_text = text_cleaned.astype(str).apply(clean_empty_features) # Step 2: Make predictions with the trained model binary_preds = model.predict(processed_text) # Step 3: Get predicted probabilities for each label pred_probs = np.array(model.predict_proba(processed_text)) # Step 4: Some samples do not have any predicted labels. # For these, take the label with the highest predicted probability. predictions = fix_no_labels(binary_preds, pred_probs, model_type=\"sklearn\") # Step 5: Convert predictions to a dataframe. preds_df = pd.DataFrame(predictions, index=processed_text.index, columns=labels) preds_df[\"labels\"] = preds_df.apply(get_labels, args=(labels,), axis=1)","title":"Using a trained model"},{"location":"reference/API/API/","text":"pxtextmining API To facilitate the use of the models trained in this project, an API has been created using the FastAPI library. Users will be able to send their patient experience feedback comments to the model via the API, and will receive the predicted labels for those comments. The API has been created using FastAPI and will be deployed on RStudio Connect. The URL is not publicly available at this stage in the project. Full documentation for the API, automatically generated by FastAPI, is available at [API URL]/docs How to make an API call 1. Prepare the data in JSON format. In Python, this is a list containing as many dict s as there are comments to be predicted. Each dict has three compulsory keys: comment_id : Unique ID associated with the comment, in str format. Each Comment ID per API call must be unique. comment_text : Text to be classified, in str format. question_type : The type of question asked to elicit the comment text. Questions are different from trust to trust, but they all fall into one of three categories: what_good : Any variation on the question \"What was good about the service?\", or \"What did we do well?\" could_improve : Any variation on the question \"Please tell us about anything that we could have done better\", or \"How could we improve?\" nonspecific : Any other type of nonspecific question, e.g. \"Please can you tell us why you gave your answer?\", or \"What were you satisfied and/or dissatisfied with?\". # In Python text_data = [ { 'comment_id': '1', # The comment_id values in each dict must be unique. 'comment_text': 'This is the first comment. Nurse was great.', 'question_type': 'what_good' }, { 'comment_id': '2', 'comment_text': 'This is the second comment. The ward was freezing.', 'question_type': 'could_improve' }, { 'comment_id': '3', 'comment_text': '', # This comment is an empty string. 'question_type': 'nonspecific' } ] # In R library(jsonlite) comment_id <- c(\"1\", \"2\", \"3\") comment_text <- c( \"This is the first comment. Nurse was great.\", \"This is the second comment. The ward was freezing.\", \"\" ) question_type <- c(\"what_good\", \"could_improve\", \"nonspecific\") df <- data.frame(comment_id, comment_text, question_type) text_data <- toJSON(df) 2. Send the JSON containing the text data to the predict_multilabel endpoint. In python, this can be done using the requests library. # In Python import requests url = \"API_URL_GOES_HERE\" headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'} response = requests.post(f\"{url}/predict_multilabel\", json = text_data, headers = headers) # In R library(httr) r <- POST( url = \"API_URL_GOES_HERE\", body = text_data, encode = \"json\", add_headers( \"User-Agent\" = \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36\", \"Content-Type\" = \"application/json\" ) ) 3. After waiting for the data to be processed and passed through the machine learning model, receive predicted labels at the same endpoint, in the example format below. Note that the comment with blank text, with comment_id 3, was assigned the label 'Labelling not possible' as it would have been stripped out during preprocessing. # In Python print(response.json()) # Output below [ { 'comment_id': '1', 'comment_text': 'This is the first comment. Nurse was great.', 'labels': ['Non-specific praise for staff']} , { 'comment_id': '2', 'comment_text': 'This is the second comment. The ward was freezing.', 'labels': ['Sensory experience']} , { 'comment_id': '3', 'comment_text': '', 'labels': ['Labelling not possible'] } ] # In R r_parsed = fromJSON(content(r, \"text\"))","title":"pxtextmining API"},{"location":"reference/API/API/#pxtextmining-api","text":"To facilitate the use of the models trained in this project, an API has been created using the FastAPI library. Users will be able to send their patient experience feedback comments to the model via the API, and will receive the predicted labels for those comments. The API has been created using FastAPI and will be deployed on RStudio Connect. The URL is not publicly available at this stage in the project. Full documentation for the API, automatically generated by FastAPI, is available at [API URL]/docs","title":"pxtextmining API"},{"location":"reference/API/API/#how-to-make-an-api-call","text":"1. Prepare the data in JSON format. In Python, this is a list containing as many dict s as there are comments to be predicted. Each dict has three compulsory keys: comment_id : Unique ID associated with the comment, in str format. Each Comment ID per API call must be unique. comment_text : Text to be classified, in str format. question_type : The type of question asked to elicit the comment text. Questions are different from trust to trust, but they all fall into one of three categories: what_good : Any variation on the question \"What was good about the service?\", or \"What did we do well?\" could_improve : Any variation on the question \"Please tell us about anything that we could have done better\", or \"How could we improve?\" nonspecific : Any other type of nonspecific question, e.g. \"Please can you tell us why you gave your answer?\", or \"What were you satisfied and/or dissatisfied with?\". # In Python text_data = [ { 'comment_id': '1', # The comment_id values in each dict must be unique. 'comment_text': 'This is the first comment. Nurse was great.', 'question_type': 'what_good' }, { 'comment_id': '2', 'comment_text': 'This is the second comment. The ward was freezing.', 'question_type': 'could_improve' }, { 'comment_id': '3', 'comment_text': '', # This comment is an empty string. 'question_type': 'nonspecific' } ] # In R library(jsonlite) comment_id <- c(\"1\", \"2\", \"3\") comment_text <- c( \"This is the first comment. Nurse was great.\", \"This is the second comment. The ward was freezing.\", \"\" ) question_type <- c(\"what_good\", \"could_improve\", \"nonspecific\") df <- data.frame(comment_id, comment_text, question_type) text_data <- toJSON(df) 2. Send the JSON containing the text data to the predict_multilabel endpoint. In python, this can be done using the requests library. # In Python import requests url = \"API_URL_GOES_HERE\" headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'} response = requests.post(f\"{url}/predict_multilabel\", json = text_data, headers = headers) # In R library(httr) r <- POST( url = \"API_URL_GOES_HERE\", body = text_data, encode = \"json\", add_headers( \"User-Agent\" = \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36\", \"Content-Type\" = \"application/json\" ) ) 3. After waiting for the data to be processed and passed through the machine learning model, receive predicted labels at the same endpoint, in the example format below. Note that the comment with blank text, with comment_id 3, was assigned the label 'Labelling not possible' as it would have been stripped out during preprocessing. # In Python print(response.json()) # Output below [ { 'comment_id': '1', 'comment_text': 'This is the first comment. Nurse was great.', 'labels': ['Non-specific praise for staff']} , { 'comment_id': '2', 'comment_text': 'This is the second comment. The ward was freezing.', 'labels': ['Sensory experience']} , { 'comment_id': '3', 'comment_text': '', 'labels': ['Labelling not possible'] } ] # In R r_parsed = fromJSON(content(r, \"text\"))","title":"How to make an API call"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/","text":"factory_data_load_and_split df = load_multilabel_data ( dataset , target = 'major_categories' ) module-attribute merge_categories ( df , new_cat , cats_to_merge ) Merges categories together in a dataset. Assumes all categories are all in the right format, one hot encoded with int values. Parameters: Name Type Description Default df pd . DataFrame DataFrame with labelled data. required new_cat str Name for new column of merged data. required cats_to_merge list List containing columns to be merged. required Returns: Type Description pd . DataFrame DataFrame with new columns bert_data_to_dataset ( X , Y = None , max_length = 150 , model_name = 'distilbert-base-uncased' , additional_features = False ) This function converts a dataframe into a format that can be utilised by a transformer model. If Y is provided then it returns a TensorFlow dataset for training the model. If Y is not provided, then it returns a dict which can be used to make predictions by an already trained model. Parameters: Name Type Description Default X pd . DataFrame Data to be converted to text data. Text should be in column 'FFT answer', FFT question should be in column 'FFT_q_standardised'. required Y pd . DataFrame One Hot Encoded targets. Defaults to None. None max_length int Maximum length of text to be encoded. Defaults to 150. 150 model_name str Type of transformer model. Defaults to 'distilbert-base-uncased'. 'distilbert-base-uncased' additional_features bool Whether additional features are to be included, currently this is only question type in 'FFT_q_standardised' column. Defaults to False. False Returns: Type Description tf.data.Dataset OR dict tf.data.Dataset if Y is provided, dict otherwise. load_multilabel_data ( filename , target = 'major_categories' ) Function for loading the multilabel dataset, converting it from csv to pd.DataFrame. Conducts some basic preprocessing, including standardisation of the question types, calculation of text length, and drops rows with no labels. Depending on selected target , returned dataframe contains different columns. Parameters: Name Type Description Default filename str Path to file containing multilabel data, in csv format required target str Options are 'minor_categories', 'major_categories', or 'sentiment'. Defaults to 'major_categories'. 'major_categories' Returns: Type Description pd . DataFrame DataFrame containing the columns 'FFT categorical answer', 'FFT question', and 'FFT answer'. Also conducts some clean_empty_features ( text_dataframe ) Replaces all empty whitespaces in a dataframe with np.NaN. Parameters: Name Type Description Default text_dataframe pd . DataFrame DataFrame containing text data with labels. required Returns: Type Description pd . DataFrame DataFrame with all empty whitespaces replaced with np.NaN onehot ( df , col_to_onehot ) Function to one-hot encode specified columns in a dataframe. Parameters: Name Type Description Default df pd . DataFrame DataFrame containing data to be one-hot encoded required col_to_onehot list List of column names to be one-hot encoded required Returns: Type Description pd . DataFrame One-hot encoded data process_data ( df , target , preprocess_text = True , additional_features = False ) Utilises remove_punc_and_nums and clean_empty_features functions to clean the text data and drop any rows that are only whitespace after cleaning. Also fills one-hot encoded columns with 0s rather than NaNs so that Y target is not sparse. Parameters: Name Type Description Default df pd . DataFrame DataFrame containing text data, any additional features, and targets required target list List of column names of targets required preprocess_text bool Whether or not text is to be processed with remove_punc_and_nums. If utilising an sklearn model then should be True. If utilising transformer-based BERT model then should be set to False. Defaults to True. True additional_features bool Whether or not 'question type' feature should be included. Defaults to False. False Returns: Type Description tuple Tuple containing two pd.DataFrames. The first contains the X features (text, with or without question type depending on additional_features), the second contains the one-hot encoded Y targets process_and_split_data ( df , target , preprocess_text = True , additional_features = False , random_state = 42 ) Combines the process_multilabel_data and train_test_split functions into one function Parameters: Name Type Description Default df pd . DataFrame DataFrame containing text data, any additional features, and targets required target list List of column names of targets required preprocess_text bool Whether or not text is to be processed with remove_punc_and_nums. If utilising an sklearn model then should be True. If utilising transformer-based BERT model then should be set to False. Defaults to True. True additional_features bool Whether or not 'question type' feature should be included. Defaults to False. False random_state int Controls the shuffling applied to the data before applying the split. Enables reproducible output across multiple function calls. Defaults to 42. 42 Returns: Type Description list List containing train-test split of preprocessed X features and Y targets. remove_punc_and_nums ( text ) Function to conduct basic preprocessing of text, removing punctuation and numbers, converting all text to lowercase, removing trailing whitespace. Parameters: Name Type Description Default text str Str containing the text to be cleaned required Returns: Type Description str Cleaned text, all lowercased with no punctuation, numbers or trailing whitespace.","title":"Factory data load and split"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split","text":"","title":"factory_data_load_and_split"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.df","text":"","title":"df"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.merge_categories","text":"Merges categories together in a dataset. Assumes all categories are all in the right format, one hot encoded with int values. Parameters: Name Type Description Default df pd . DataFrame DataFrame with labelled data. required new_cat str Name for new column of merged data. required cats_to_merge list List containing columns to be merged. required Returns: Type Description pd . DataFrame DataFrame with new columns","title":"merge_categories()"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.bert_data_to_dataset","text":"This function converts a dataframe into a format that can be utilised by a transformer model. If Y is provided then it returns a TensorFlow dataset for training the model. If Y is not provided, then it returns a dict which can be used to make predictions by an already trained model. Parameters: Name Type Description Default X pd . DataFrame Data to be converted to text data. Text should be in column 'FFT answer', FFT question should be in column 'FFT_q_standardised'. required Y pd . DataFrame One Hot Encoded targets. Defaults to None. None max_length int Maximum length of text to be encoded. Defaults to 150. 150 model_name str Type of transformer model. Defaults to 'distilbert-base-uncased'. 'distilbert-base-uncased' additional_features bool Whether additional features are to be included, currently this is only question type in 'FFT_q_standardised' column. Defaults to False. False Returns: Type Description tf.data.Dataset OR dict tf.data.Dataset if Y is provided, dict otherwise.","title":"bert_data_to_dataset()"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.load_multilabel_data","text":"Function for loading the multilabel dataset, converting it from csv to pd.DataFrame. Conducts some basic preprocessing, including standardisation of the question types, calculation of text length, and drops rows with no labels. Depending on selected target , returned dataframe contains different columns. Parameters: Name Type Description Default filename str Path to file containing multilabel data, in csv format required target str Options are 'minor_categories', 'major_categories', or 'sentiment'. Defaults to 'major_categories'. 'major_categories' Returns: Type Description pd . DataFrame DataFrame containing the columns 'FFT categorical answer', 'FFT question', and 'FFT answer'. Also conducts some","title":"load_multilabel_data()"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.clean_empty_features","text":"Replaces all empty whitespaces in a dataframe with np.NaN. Parameters: Name Type Description Default text_dataframe pd . DataFrame DataFrame containing text data with labels. required Returns: Type Description pd . DataFrame DataFrame with all empty whitespaces replaced with np.NaN","title":"clean_empty_features()"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.onehot","text":"Function to one-hot encode specified columns in a dataframe. Parameters: Name Type Description Default df pd . DataFrame DataFrame containing data to be one-hot encoded required col_to_onehot list List of column names to be one-hot encoded required Returns: Type Description pd . DataFrame One-hot encoded data","title":"onehot()"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.process_data","text":"Utilises remove_punc_and_nums and clean_empty_features functions to clean the text data and drop any rows that are only whitespace after cleaning. Also fills one-hot encoded columns with 0s rather than NaNs so that Y target is not sparse. Parameters: Name Type Description Default df pd . DataFrame DataFrame containing text data, any additional features, and targets required target list List of column names of targets required preprocess_text bool Whether or not text is to be processed with remove_punc_and_nums. If utilising an sklearn model then should be True. If utilising transformer-based BERT model then should be set to False. Defaults to True. True additional_features bool Whether or not 'question type' feature should be included. Defaults to False. False Returns: Type Description tuple Tuple containing two pd.DataFrames. The first contains the X features (text, with or without question type depending on additional_features), the second contains the one-hot encoded Y targets","title":"process_data()"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.process_and_split_data","text":"Combines the process_multilabel_data and train_test_split functions into one function Parameters: Name Type Description Default df pd . DataFrame DataFrame containing text data, any additional features, and targets required target list List of column names of targets required preprocess_text bool Whether or not text is to be processed with remove_punc_and_nums. If utilising an sklearn model then should be True. If utilising transformer-based BERT model then should be set to False. Defaults to True. True additional_features bool Whether or not 'question type' feature should be included. Defaults to False. False random_state int Controls the shuffling applied to the data before applying the split. Enables reproducible output across multiple function calls. Defaults to 42. 42 Returns: Type Description list List containing train-test split of preprocessed X features and Y targets.","title":"process_and_split_data()"},{"location":"reference/pxtextmining/factories/factory_data_load_and_split/#pxtextmining.factories.factory_data_load_and_split.remove_punc_and_nums","text":"Function to conduct basic preprocessing of text, removing punctuation and numbers, converting all text to lowercase, removing trailing whitespace. Parameters: Name Type Description Default text str Str containing the text to be cleaned required Returns: Type Description str Cleaned text, all lowercased with no punctuation, numbers or trailing whitespace.","title":"remove_punc_and_nums()"},{"location":"reference/pxtextmining/factories/factory_model_performance/","text":"factory_model_performance get_dummy_model ( x_train , y_train ) Creates dummy model that randomly predicts labels, fitted on the training data. Parameters: Name Type Description Default x_train pd . DataFrame Input features. required y_train pd . DataFrame Target values. required Returns: Type Description sklearn . dummy . DummyClassifier Trained dummy classifier. get_multiclass_metrics ( x_test , y_test , labels , random_state , model , additional_features , training_time = None ) Creates a string detailing various performance metrics for a multiclass model, which can then be written to a text file. Parameters: Name Type Description Default x_test pd . DataFrame DataFrame containing test dataset features required y_test pd . DataFrame DataFrame containing test dataset true target values required labels list List containing the target labels required random_state int Seed used to control the shuffling of the data, to enable reproducible results. required model tf.keras or sklearn model Trained estimator. required additional_features bool Whether or not additional features (e.g. question type) have been included in training the model. Defaults to False. required training_time str Amount of time taken for model to train. Defaults to None. None Raises: Type Description ValueError Only models built with sklearn or tensorflow are allowed. Returns: Type Description str String containing the model architecture/hyperparameters, random state used for the train test split, and classification report. get_multilabel_metrics ( x_test , y_test , labels , random_state , model_type , model , training_time = None , additional_features = False , already_encoded = False ) Creates a string detailing various performance metrics for a multilabel model, which can then be written to a text file. Parameters: Name Type Description Default x_test pd . DataFrame DataFrame containing test dataset features required y_test pd . DataFrame DataFrame containing test dataset true target values required labels list List containing the target labels required random_state int Seed used to control the shuffling of the data, to enable reproducible results. required model_type str Type of model used. Options are 'bert', 'tf', or 'sklearn'. Defaults to None. required model tf.keras or sklearn model Trained estimator. required training_time str Amount of time taken for model to train. Defaults to None. None additional_features bool Whether or not additional features (e.g. question type) have been included in training the model. Defaults to False. False already_encoded bool Whether or not, if a bert model was used, x_test has already been encoded. Defaults to False. False Raises: Type Description ValueError Only model_type 'bert', 'tf' or 'sklearn' are allowed. Returns: Type Description str String containing the model architecture/hyperparameters, random state used for the train test split, and performance metrics including: exact accuracy, hamming loss, macro jaccard score, and classification report. get_accuracy_per_class ( y_test , pred ) Function to produce accuracy per class for the predicted categories, compared against real values. Parameters: Name Type Description Default y_test pd . Series Test data (real target values). required pred pd . Series Predicted target values. required Returns: Type Description pd . DataFrame The computed accuracy per class metrics for the model. parse_metrics_file ( metrics_file , labels ) Reads performance metrics files that are written by factory_write_results.write_multilabel_models_and_metrics . Creates a pd.DataFrame with the precision, recall, f1_score, and support for each label, which can be filtered and sorted more easily. Parameters: Name Type Description Default metrics_file str Path to the metrics file to be parsed. required labels list List of the target labels used in the metrics file. required Returns: Type Description pd . DataFrame DataFrame containing the precision, recall, f1_score, and support for each label, as detailed in the performance metrics file.","title":"Factory model performance"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance","text":"","title":"factory_model_performance"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance.get_dummy_model","text":"Creates dummy model that randomly predicts labels, fitted on the training data. Parameters: Name Type Description Default x_train pd . DataFrame Input features. required y_train pd . DataFrame Target values. required Returns: Type Description sklearn . dummy . DummyClassifier Trained dummy classifier.","title":"get_dummy_model()"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance.get_multiclass_metrics","text":"Creates a string detailing various performance metrics for a multiclass model, which can then be written to a text file. Parameters: Name Type Description Default x_test pd . DataFrame DataFrame containing test dataset features required y_test pd . DataFrame DataFrame containing test dataset true target values required labels list List containing the target labels required random_state int Seed used to control the shuffling of the data, to enable reproducible results. required model tf.keras or sklearn model Trained estimator. required additional_features bool Whether or not additional features (e.g. question type) have been included in training the model. Defaults to False. required training_time str Amount of time taken for model to train. Defaults to None. None Raises: Type Description ValueError Only models built with sklearn or tensorflow are allowed. Returns: Type Description str String containing the model architecture/hyperparameters, random state used for the train test split, and classification report.","title":"get_multiclass_metrics()"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance.get_multilabel_metrics","text":"Creates a string detailing various performance metrics for a multilabel model, which can then be written to a text file. Parameters: Name Type Description Default x_test pd . DataFrame DataFrame containing test dataset features required y_test pd . DataFrame DataFrame containing test dataset true target values required labels list List containing the target labels required random_state int Seed used to control the shuffling of the data, to enable reproducible results. required model_type str Type of model used. Options are 'bert', 'tf', or 'sklearn'. Defaults to None. required model tf.keras or sklearn model Trained estimator. required training_time str Amount of time taken for model to train. Defaults to None. None additional_features bool Whether or not additional features (e.g. question type) have been included in training the model. Defaults to False. False already_encoded bool Whether or not, if a bert model was used, x_test has already been encoded. Defaults to False. False Raises: Type Description ValueError Only model_type 'bert', 'tf' or 'sklearn' are allowed. Returns: Type Description str String containing the model architecture/hyperparameters, random state used for the train test split, and performance metrics including: exact accuracy, hamming loss, macro jaccard score, and classification report.","title":"get_multilabel_metrics()"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance.get_accuracy_per_class","text":"Function to produce accuracy per class for the predicted categories, compared against real values. Parameters: Name Type Description Default y_test pd . Series Test data (real target values). required pred pd . Series Predicted target values. required Returns: Type Description pd . DataFrame The computed accuracy per class metrics for the model.","title":"get_accuracy_per_class()"},{"location":"reference/pxtextmining/factories/factory_model_performance/#pxtextmining.factories.factory_model_performance.parse_metrics_file","text":"Reads performance metrics files that are written by factory_write_results.write_multilabel_models_and_metrics . Creates a pd.DataFrame with the precision, recall, f1_score, and support for each label, which can be filtered and sorted more easily. Parameters: Name Type Description Default metrics_file str Path to the metrics file to be parsed. required labels list List of the target labels used in the metrics file. required Returns: Type Description pd . DataFrame DataFrame containing the precision, recall, f1_score, and support for each label, as detailed in the performance metrics file.","title":"parse_metrics_file()"},{"location":"reference/pxtextmining/factories/factory_pipeline/","text":"factory_pipeline model_name = model_name module-attribute create_sklearn_pipeline_sentiment ( model_type , num_classes , tokenizer = None , additional_features = False ) Creates sklearn pipeline and hyperparameter grid for searching, for a multiclass target. Parameters: Name Type Description Default model_type str Allows for selection of different estimators. Permitted values are \"svm\" (Support Vector Classifier), or \"xgb\" (XGBoost). required num_classes int Number of target classes. required tokenizer str Allows for selection of \"spacy\" tokenizer. Defaults to None, which is the default sklearn tokenizer None additional_features bool Whether or not additional features (question type, text length) are to be included in the features fed into the model. Defaults to True. False Returns: Type Description tuple Tuple containing the sklearn.pipeline.Pipeline with the selected estimator, and a dict containing the hyperparameters to be tuned. create_bert_model ( Y_train , model_name = model_name , max_length = 150 , multilabel = True ) Creates Transformer based model trained on text data, with last layer added on for multilabel classification task. Number of neurons in last layer depends on number of labels in Y target. Parameters: Name Type Description Default Y_train pd . DataFrame DataFrame containing one-hot encoded targets required model_name str Type of pretrained transformer model to load. Defaults to model_name set in pxtextmining.params model_name max_length int Maximum length of text to be passed through transformer model. Defaults to 150. 150 multilabel Bool Whether the target is multilabel or not. If set to False, target is multiclass. Defaults to True. True Returns: Type Description tensorflow . keras . models . Model Compiled Tensforflow Keras model with pretrained transformer layers and last layer suited for multilabel classification task create_bert_model_additional_features ( Y_train , model_name = model_name , max_length = 150 , multilabel = True ) Creates Transformer based model trained on text data, concatenated with an additional Dense layer taking additional inputs, with last layer added on for multilabel classification task. Number of neurons in last layer depends on number of labels in Y target. Parameters: Name Type Description Default Y_train pd . DataFrame DataFrame containing one-hot encoded targets required model_name str Type of pretrained transformer model to load. Defaults to model_name set in pxtextmining.params model_name max_length int Maximum length of text to be passed through transformer model. Defaults to 150. 150 multilabel Bool Whether the target is multilabel or not. If set to False, target is multiclass. Defaults to True. True Returns: Type Description tensorflow . keras . models . Model Compiled Tensforflow Keras model with pretrained transformer layers, three question_type inputs passed through a Dense layer, and last layer suited for multilabel classification task train_bert_model ( train_dataset , val_dataset , model , class_weights_dict = None , epochs = 30 ) Trains compiled transformer model with early stopping. Parameters: Name Type Description Default train_dataset tf . data . Dataset Train dataset, tokenized with huggingface tokenizer, in tf.data.Dataset format required val_dataset tf . data . Dataset Validation dataset, tokenized with huggingface tokenizer, in tf.data.Dataset format required model tf . keras . models . Model Compiled transformer model with additional layers for specific task. required class_weights_dict dict Dict containing class weights for each target class. Defaults to None. None epochs int Number of epochs to train model for. Defaults to 30. 30 Returns: Type Description tuple Tuple containing trained model and the training time as a str. calculating_class_weights ( y_true ) Function for calculating class weights for target classes, to be used when fitting a model. Parameters: Name Type Description Default y_true pd . DataFrame Dataset containing onehot encoded multilabel targets required Returns: Type Description dict Dict containing calculated class weights for each target label. create_tf_model ( vocab_size = None , embedding_size = 100 ) Creates LSTM multilabel classification model using tensorflow keras, with a layer of outputs matching what is currently the number of major_categories. Parameters: Name Type Description Default vocab_size int Number of different words in vocabulary, as derived from tokenization process. Defaults to None. None embedding_size int Size of embedding dimension to be output by embedding layer. Defaults to 100. 100 Returns: Type Description tf . keras . models . Model Compiled tensorflow keras LSTM model. train_tf_model ( X_train , Y_train , model , class_weights_dict = None ) Trains tensorflow keras model. Some overlap with train_bert_model, could probably be merged. Parameters: Name Type Description Default X_train pd . DataFrame DataFrame containing tokenized text features. required Y_train pd . DataFrame DataFrame containing one-hot encoded multilabel targets. required model tf . keras . models . Model Compiled tensorflow keras model. required class_weights_dict dict Dict containing class weights for target classes. Defaults to None. None Returns: Type Description tuple Tuple containing trained model and the training time as a str. create_sklearn_vectorizer ( tokenizer = None ) Creates vectorizer for use with sklearn models, either using sklearn tokenizer or the spacy tokenizer Parameters: Name Type Description Default tokenizer str Enables selection of spacy tokenizer. Defaults to None, which is sklearn default tokenizer. None Returns: Type Description sklearn . feature_extraction . text . TfidfVectorizer sklearn TfidfVectorizer with either spacy or sklearn tokenizer create_sklearn_pipeline ( model_type , tokenizer = None , additional_features = True ) Creates sklearn pipeline and hyperparameter grid for searching, depending on model_type selected. Parameters: Name Type Description Default model_type str Allows for selection of different estimators. Permitted values are \"mnb\" (Multinomial Naive Bayes), \"knn\" (K Nearest Neighbours), \"svm\" (Support Vector Classifier), or \"rfc\" (Random Forest Classifier). required tokenizer str Allows for selection of \"spacy\" tokenizer. Defaults to None, which is the default sklearn tokenizer None additional_features bool Whether or not additional features (question type, text length) are to be included in the features fed into the model. Defaults to True. True Returns: Type Description tuple Tuple containing the sklearn.pipeline.Pipeline with the selected estimator, and a dict containing the hyperparameters to be tuned. search_sklearn_pipelines ( X_train , Y_train , models_to_try , target = None , additional_features = True ) Iterates through selected estimators, instantiating the relevant sklearn pipelines and searching for the optimum hyperparameters. Parameters: Name Type Description Default X_train pd . DataFrame DataFrame containing the features to be fed into the estimator required Y_train pd . DataFrame DataFrame containing the targets required models_to_try list List containing the estimator types to be tried. Permitted values are \"mnb\" (Multinomial Naive Bayes), \"knn\" (K Nearest Neighbours), \"svm\" (Support Vector Classifier), or \"rfc\" (Random Forest Classifier). required additional_features bool Whether or not additional features (question type, text length) are to be included in the features fed into the model. Defaults to True. True Raises: Type Description ValueError If model_type includes value other than permitted values \"mnb\", \"knn\", \"svm\", or \"rfc\" Returns: Type Description tuple Tuple containing: a list containing the refitted pipelines with the best hyperparameters identified in the search, and a list containing the training times for each of the pipelines. create_and_train_svc_model ( X_train , Y_train ) Creates pipeline with a Support Vector Classifier using specific hyperparameters identified through previous gridsearching. Parameters: Name Type Description Default X_train pd . DataFrame DataFrame containing the features to be fed into the estimator required Y_train pd . DataFrame DataFrame containing the targets required Returns: Type Description tuple Tuple containing: a fitted pipeline with a MultiOutputClassifier utilising a Support Vector Classifier estimator, and a str of the training time taken for the fitting of the pipeline.","title":"Factory pipeline"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline","text":"","title":"factory_pipeline"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.model_name","text":"","title":"model_name"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_sklearn_pipeline_sentiment","text":"Creates sklearn pipeline and hyperparameter grid for searching, for a multiclass target. Parameters: Name Type Description Default model_type str Allows for selection of different estimators. Permitted values are \"svm\" (Support Vector Classifier), or \"xgb\" (XGBoost). required num_classes int Number of target classes. required tokenizer str Allows for selection of \"spacy\" tokenizer. Defaults to None, which is the default sklearn tokenizer None additional_features bool Whether or not additional features (question type, text length) are to be included in the features fed into the model. Defaults to True. False Returns: Type Description tuple Tuple containing the sklearn.pipeline.Pipeline with the selected estimator, and a dict containing the hyperparameters to be tuned.","title":"create_sklearn_pipeline_sentiment()"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_bert_model","text":"Creates Transformer based model trained on text data, with last layer added on for multilabel classification task. Number of neurons in last layer depends on number of labels in Y target. Parameters: Name Type Description Default Y_train pd . DataFrame DataFrame containing one-hot encoded targets required model_name str Type of pretrained transformer model to load. Defaults to model_name set in pxtextmining.params model_name max_length int Maximum length of text to be passed through transformer model. Defaults to 150. 150 multilabel Bool Whether the target is multilabel or not. If set to False, target is multiclass. Defaults to True. True Returns: Type Description tensorflow . keras . models . Model Compiled Tensforflow Keras model with pretrained transformer layers and last layer suited for multilabel classification task","title":"create_bert_model()"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_bert_model_additional_features","text":"Creates Transformer based model trained on text data, concatenated with an additional Dense layer taking additional inputs, with last layer added on for multilabel classification task. Number of neurons in last layer depends on number of labels in Y target. Parameters: Name Type Description Default Y_train pd . DataFrame DataFrame containing one-hot encoded targets required model_name str Type of pretrained transformer model to load. Defaults to model_name set in pxtextmining.params model_name max_length int Maximum length of text to be passed through transformer model. Defaults to 150. 150 multilabel Bool Whether the target is multilabel or not. If set to False, target is multiclass. Defaults to True. True Returns: Type Description tensorflow . keras . models . Model Compiled Tensforflow Keras model with pretrained transformer layers, three question_type inputs passed through a Dense layer, and last layer suited for multilabel classification task","title":"create_bert_model_additional_features()"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.train_bert_model","text":"Trains compiled transformer model with early stopping. Parameters: Name Type Description Default train_dataset tf . data . Dataset Train dataset, tokenized with huggingface tokenizer, in tf.data.Dataset format required val_dataset tf . data . Dataset Validation dataset, tokenized with huggingface tokenizer, in tf.data.Dataset format required model tf . keras . models . Model Compiled transformer model with additional layers for specific task. required class_weights_dict dict Dict containing class weights for each target class. Defaults to None. None epochs int Number of epochs to train model for. Defaults to 30. 30 Returns: Type Description tuple Tuple containing trained model and the training time as a str.","title":"train_bert_model()"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.calculating_class_weights","text":"Function for calculating class weights for target classes, to be used when fitting a model. Parameters: Name Type Description Default y_true pd . DataFrame Dataset containing onehot encoded multilabel targets required Returns: Type Description dict Dict containing calculated class weights for each target label.","title":"calculating_class_weights()"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_tf_model","text":"Creates LSTM multilabel classification model using tensorflow keras, with a layer of outputs matching what is currently the number of major_categories. Parameters: Name Type Description Default vocab_size int Number of different words in vocabulary, as derived from tokenization process. Defaults to None. None embedding_size int Size of embedding dimension to be output by embedding layer. Defaults to 100. 100 Returns: Type Description tf . keras . models . Model Compiled tensorflow keras LSTM model.","title":"create_tf_model()"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.train_tf_model","text":"Trains tensorflow keras model. Some overlap with train_bert_model, could probably be merged. Parameters: Name Type Description Default X_train pd . DataFrame DataFrame containing tokenized text features. required Y_train pd . DataFrame DataFrame containing one-hot encoded multilabel targets. required model tf . keras . models . Model Compiled tensorflow keras model. required class_weights_dict dict Dict containing class weights for target classes. Defaults to None. None Returns: Type Description tuple Tuple containing trained model and the training time as a str.","title":"train_tf_model()"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_sklearn_vectorizer","text":"Creates vectorizer for use with sklearn models, either using sklearn tokenizer or the spacy tokenizer Parameters: Name Type Description Default tokenizer str Enables selection of spacy tokenizer. Defaults to None, which is sklearn default tokenizer. None Returns: Type Description sklearn . feature_extraction . text . TfidfVectorizer sklearn TfidfVectorizer with either spacy or sklearn tokenizer","title":"create_sklearn_vectorizer()"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_sklearn_pipeline","text":"Creates sklearn pipeline and hyperparameter grid for searching, depending on model_type selected. Parameters: Name Type Description Default model_type str Allows for selection of different estimators. Permitted values are \"mnb\" (Multinomial Naive Bayes), \"knn\" (K Nearest Neighbours), \"svm\" (Support Vector Classifier), or \"rfc\" (Random Forest Classifier). required tokenizer str Allows for selection of \"spacy\" tokenizer. Defaults to None, which is the default sklearn tokenizer None additional_features bool Whether or not additional features (question type, text length) are to be included in the features fed into the model. Defaults to True. True Returns: Type Description tuple Tuple containing the sklearn.pipeline.Pipeline with the selected estimator, and a dict containing the hyperparameters to be tuned.","title":"create_sklearn_pipeline()"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.search_sklearn_pipelines","text":"Iterates through selected estimators, instantiating the relevant sklearn pipelines and searching for the optimum hyperparameters. Parameters: Name Type Description Default X_train pd . DataFrame DataFrame containing the features to be fed into the estimator required Y_train pd . DataFrame DataFrame containing the targets required models_to_try list List containing the estimator types to be tried. Permitted values are \"mnb\" (Multinomial Naive Bayes), \"knn\" (K Nearest Neighbours), \"svm\" (Support Vector Classifier), or \"rfc\" (Random Forest Classifier). required additional_features bool Whether or not additional features (question type, text length) are to be included in the features fed into the model. Defaults to True. True Raises: Type Description ValueError If model_type includes value other than permitted values \"mnb\", \"knn\", \"svm\", or \"rfc\" Returns: Type Description tuple Tuple containing: a list containing the refitted pipelines with the best hyperparameters identified in the search, and a list containing the training times for each of the pipelines.","title":"search_sklearn_pipelines()"},{"location":"reference/pxtextmining/factories/factory_pipeline/#pxtextmining.factories.factory_pipeline.create_and_train_svc_model","text":"Creates pipeline with a Support Vector Classifier using specific hyperparameters identified through previous gridsearching. Parameters: Name Type Description Default X_train pd . DataFrame DataFrame containing the features to be fed into the estimator required Y_train pd . DataFrame DataFrame containing the targets required Returns: Type Description tuple Tuple containing: a fitted pipeline with a MultiOutputClassifier utilising a Support Vector Classifier estimator, and a str of the training time taken for the fitting of the pipeline.","title":"create_and_train_svc_model()"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/","text":"factory_predict_unlabelled_text process_text ( text ) Enacts same text preprocessing as is found in factory_data_load_and_split when creating training data. Converts to string, removes trailing whitespaces, null values, punctuation and numbers. Converts to lowercase. Parameters: Name Type Description Default text pd . Series Series containing data to be cleaned. required Returns: Type Description pd . Series Processed text data predict_multilabel_sklearn ( data , model , labels = minor_cats , additional_features = False , label_fix = True , enhance_with_probs = True ) Conducts basic preprocessing to remove punctuation and numbers. Utilises a pretrained sklearn machine learning model to make multilabel predictions on the cleaned text. Also takes the class with the highest predicted probability as the predicted class in cases where no class has been predicted, if fix_no_labels = True. Parameters: Name Type Description Default text pd.Series OR pd.DataFrame DataFrame or Series containing data to be processed and utilised for predictions. Must be DataFrame with columns 'FFT answer' and 'FFT_q_standardised' if additional_features = True required model sklearn . base Trained sklearn estimator able to perform multilabel classification. required labels list List containing target labels. Defaults to major_cats. minor_cats additional_features bool Whether or not FFT_q_standardised is included in data. Defaults to False. False label_fix bool Whether or not the class with the highest probability is taken as the predicted class in cases where no classes are predicted. Defaults to True. True enhance_with_probs bool Whether or not to enhance predicted classes with predictions utilising the model's outputted probabilities. True Returns: Type Description pd . DataFrame DataFrame containing one hot encoded predictions, and a column with a list of the predicted labels. predict_multiclass_bert ( x , model , additional_features , already_encoded ) Makes multiclass predictions using a transformer-based model. Can encode the data if not already encoded. Parameters: Name Type Description Default x pd . DataFrame DataFrame containing features to be passed through model. required model tf . keras . models . Model Pretrained transformer based model in tensorflow keras. required additional_features bool Whether or not additional features (e.g. question type) are included. Defaults to False. required already_encoded bool Whether or not the input data needs to be encoded. Defaults to False. required Returns: Type Description np . array Predicted labels in one-hot encoded format. predict_with_probs ( x , model , labels ) Given a trained model and some features, makes predictions based on the model's outputted probabilities using the model.predict_proba function. Any label with a predicted probability over 0.5 is taken as the predicted label. If no labels are over 0.5 probability then the label with the highest probability is taken. Converts into one-hot encoded format (similar to what model.predict would output). Currently only works with sklearn models. Parameters: Name Type Description Default x pd . DataFrame Features to be used to make the prediction. required model sklearn . base Trained sklearn multilabel classifier. required labels list List of labels for the categories to be predicted. required Returns: Type Description np . array Predicted labels in one hot encoded format based on model probability estimates. get_probabilities ( label_series , labels , predicted_probabilities , model_type ) Given a pd.Series containing labels, the list of labels, and a model's outputted predicted_probabilities for each label, create a dictionary containing the label and the predicted probability of that label. Parameters: Name Type Description Default label_series pd . Series Series containing labels in the format ['label_one', 'label_two'] required labels list List of the label names required predicted_probabilities np . array Predicted probabilities for each label required model_type str Model architecture, if sklearn or otherwise. required Returns: Type Description pd . Series Series, each line containing a dict with the predicted probabilities for each label. get_labels ( row , labels ) Given a one-hot encoded row of predictions from a dataframe, returns a list containing the actual predicted labels as a str . Parameters: Name Type Description Default row pd . DataFrame Row in a DataFrame containing one-hot encoded predicted labels. required labels list List containing all the target labels, which should also be columns in the dataframe. required Returns: Type Description list List of the labels that have been predicted for the given text. predict_with_bert ( data , model , max_length = 150 , additional_features = False , already_encoded = False ) Makes predictions using a transformer-based model. Can encode the data if not already encoded. Parameters: Name Type Description Default data pd . DataFrame DataFrame containing features to be passed through model. required model tf . keras . models . Model Pretrained transformer based model in tensorflow keras. required max_length int If encoding is required, maximum length of input text. Defaults to 150. 150 additional_features bool Whether or not additional features (e.g. question type) are included. Defaults to False. False already_encoded bool Whether or not the input data needs to be encoded. Defaults to False. False Returns: Type Description np . array Predicted probabilities for each label. fix_no_labels ( binary_preds , predicted_probs , model_type = 'sklearn' ) Function that takes in the binary predicted labels for a particular input, and the predicted probabilities for all the labels classes. Where no labels have been predicted for a particular input, takes the label with the highest predicted probability as the predicted label. Parameters: Name Type Description Default binary_preds np . array Predicted labels, in a one-hot encoded binary format. Some rows may not have any predicted labels. required predicted_probs np . array Predicted probability of each label. required model_type str Whether the model is a sklearn or tensorflow keras model; options are 'tf', 'bert', or 'sklearn. Defaults to \"sklearn\". 'sklearn' Returns: Type Description np . array Predicted labels in one-hot encoded format, with all rows containing at least one predicted label. turn_probs_into_binary ( predicted_probs ) Takes predicted probabilities (floats between 0 and 1) and converts these to binary outcomes. Scope to finetune this in later iterations of the project depending on the label and whether recall/precision is prioritised for that label. Parameters: Name Type Description Default predicted_probs np . array Array containing the predicted probabilities for each class. Shape of array corresponds to the number of inputs and the number of target classes; if shape is (100, 13) then there are 100 datapoints, and 13 target classes. Predicted probabilities should range from 0 to 1. required Returns: Type Description np . array Array containing binary outcomes for each label. Shape should remain the same as input, but values will be either 0 or 1.","title":"Factory predict unlabelled text"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text","text":"","title":"factory_predict_unlabelled_text"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.process_text","text":"Enacts same text preprocessing as is found in factory_data_load_and_split when creating training data. Converts to string, removes trailing whitespaces, null values, punctuation and numbers. Converts to lowercase. Parameters: Name Type Description Default text pd . Series Series containing data to be cleaned. required Returns: Type Description pd . Series Processed text data","title":"process_text()"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.predict_multilabel_sklearn","text":"Conducts basic preprocessing to remove punctuation and numbers. Utilises a pretrained sklearn machine learning model to make multilabel predictions on the cleaned text. Also takes the class with the highest predicted probability as the predicted class in cases where no class has been predicted, if fix_no_labels = True. Parameters: Name Type Description Default text pd.Series OR pd.DataFrame DataFrame or Series containing data to be processed and utilised for predictions. Must be DataFrame with columns 'FFT answer' and 'FFT_q_standardised' if additional_features = True required model sklearn . base Trained sklearn estimator able to perform multilabel classification. required labels list List containing target labels. Defaults to major_cats. minor_cats additional_features bool Whether or not FFT_q_standardised is included in data. Defaults to False. False label_fix bool Whether or not the class with the highest probability is taken as the predicted class in cases where no classes are predicted. Defaults to True. True enhance_with_probs bool Whether or not to enhance predicted classes with predictions utilising the model's outputted probabilities. True Returns: Type Description pd . DataFrame DataFrame containing one hot encoded predictions, and a column with a list of the predicted labels.","title":"predict_multilabel_sklearn()"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.predict_multiclass_bert","text":"Makes multiclass predictions using a transformer-based model. Can encode the data if not already encoded. Parameters: Name Type Description Default x pd . DataFrame DataFrame containing features to be passed through model. required model tf . keras . models . Model Pretrained transformer based model in tensorflow keras. required additional_features bool Whether or not additional features (e.g. question type) are included. Defaults to False. required already_encoded bool Whether or not the input data needs to be encoded. Defaults to False. required Returns: Type Description np . array Predicted labels in one-hot encoded format.","title":"predict_multiclass_bert()"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.predict_with_probs","text":"Given a trained model and some features, makes predictions based on the model's outputted probabilities using the model.predict_proba function. Any label with a predicted probability over 0.5 is taken as the predicted label. If no labels are over 0.5 probability then the label with the highest probability is taken. Converts into one-hot encoded format (similar to what model.predict would output). Currently only works with sklearn models. Parameters: Name Type Description Default x pd . DataFrame Features to be used to make the prediction. required model sklearn . base Trained sklearn multilabel classifier. required labels list List of labels for the categories to be predicted. required Returns: Type Description np . array Predicted labels in one hot encoded format based on model probability estimates.","title":"predict_with_probs()"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.get_probabilities","text":"Given a pd.Series containing labels, the list of labels, and a model's outputted predicted_probabilities for each label, create a dictionary containing the label and the predicted probability of that label. Parameters: Name Type Description Default label_series pd . Series Series containing labels in the format ['label_one', 'label_two'] required labels list List of the label names required predicted_probabilities np . array Predicted probabilities for each label required model_type str Model architecture, if sklearn or otherwise. required Returns: Type Description pd . Series Series, each line containing a dict with the predicted probabilities for each label.","title":"get_probabilities()"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.get_labels","text":"Given a one-hot encoded row of predictions from a dataframe, returns a list containing the actual predicted labels as a str . Parameters: Name Type Description Default row pd . DataFrame Row in a DataFrame containing one-hot encoded predicted labels. required labels list List containing all the target labels, which should also be columns in the dataframe. required Returns: Type Description list List of the labels that have been predicted for the given text.","title":"get_labels()"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.predict_with_bert","text":"Makes predictions using a transformer-based model. Can encode the data if not already encoded. Parameters: Name Type Description Default data pd . DataFrame DataFrame containing features to be passed through model. required model tf . keras . models . Model Pretrained transformer based model in tensorflow keras. required max_length int If encoding is required, maximum length of input text. Defaults to 150. 150 additional_features bool Whether or not additional features (e.g. question type) are included. Defaults to False. False already_encoded bool Whether or not the input data needs to be encoded. Defaults to False. False Returns: Type Description np . array Predicted probabilities for each label.","title":"predict_with_bert()"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.fix_no_labels","text":"Function that takes in the binary predicted labels for a particular input, and the predicted probabilities for all the labels classes. Where no labels have been predicted for a particular input, takes the label with the highest predicted probability as the predicted label. Parameters: Name Type Description Default binary_preds np . array Predicted labels, in a one-hot encoded binary format. Some rows may not have any predicted labels. required predicted_probs np . array Predicted probability of each label. required model_type str Whether the model is a sklearn or tensorflow keras model; options are 'tf', 'bert', or 'sklearn. Defaults to \"sklearn\". 'sklearn' Returns: Type Description np . array Predicted labels in one-hot encoded format, with all rows containing at least one predicted label.","title":"fix_no_labels()"},{"location":"reference/pxtextmining/factories/factory_predict_unlabelled_text/#pxtextmining.factories.factory_predict_unlabelled_text.turn_probs_into_binary","text":"Takes predicted probabilities (floats between 0 and 1) and converts these to binary outcomes. Scope to finetune this in later iterations of the project depending on the label and whether recall/precision is prioritised for that label. Parameters: Name Type Description Default predicted_probs np . array Array containing the predicted probabilities for each class. Shape of array corresponds to the number of inputs and the number of target classes; if shape is (100, 13) then there are 100 datapoints, and 13 target classes. Predicted probabilities should range from 0 to 1. required Returns: Type Description np . array Array containing binary outcomes for each label. Shape should remain the same as input, but values will be either 0 or 1.","title":"turn_probs_into_binary()"},{"location":"reference/pxtextmining/factories/factory_write_results/","text":"factory_write_results write_multilabel_models_and_metrics ( models , model_metrics , path ) Saves models and their associated performance metrics into a specified folder Parameters: Name Type Description Default models list List containing the trained tf.keras or sklearn models to be saved. required model_metrics list List containing the model metrics in str format required path str Path where model is to be saved. required write_model_preds ( x , y , model , labels , additional_features = True , path = 'labels.xlsx' ) Writes an Excel file to enable easier analysis of model outputs using the test set. Columns of the Excel file are: comment_id, actual_labels, predicted_labels, actual_label_probs, and predicted_label_probs. Currently only works with sklearn models. Parameters: Name Type Description Default x pd . DataFrame Features to be used to make the prediction. required y np . array Numpy array containing the targets, in one-hot encoded format required model sklearn . base Trained sklearn multilabel classifier. required labels list List of labels for the categories to be predicted. required additional_features bool Whether or not FFT_q_standardised is included in data. Defaults to True. True path str Filename for the outputted file. Defaults to 'labels.xlsx'. 'labels.xlsx' write_model_analysis ( model_name , labels , dataset , path ) Writes an Excel file with the performance metrics of each label, as well as the counts of samples for each label. Parameters: Name Type Description Default model_name str Model name used in the performance metrics file required labels list List of labels for the categories to be predicted. required dataset pd . DataFrame Original dataset before train test split required path str Filepath where model and performance metrics file are saved. required","title":"Factory write results"},{"location":"reference/pxtextmining/factories/factory_write_results/#pxtextmining.factories.factory_write_results","text":"","title":"factory_write_results"},{"location":"reference/pxtextmining/factories/factory_write_results/#pxtextmining.factories.factory_write_results.write_multilabel_models_and_metrics","text":"Saves models and their associated performance metrics into a specified folder Parameters: Name Type Description Default models list List containing the trained tf.keras or sklearn models to be saved. required model_metrics list List containing the model metrics in str format required path str Path where model is to be saved. required","title":"write_multilabel_models_and_metrics()"},{"location":"reference/pxtextmining/factories/factory_write_results/#pxtextmining.factories.factory_write_results.write_model_preds","text":"Writes an Excel file to enable easier analysis of model outputs using the test set. Columns of the Excel file are: comment_id, actual_labels, predicted_labels, actual_label_probs, and predicted_label_probs. Currently only works with sklearn models. Parameters: Name Type Description Default x pd . DataFrame Features to be used to make the prediction. required y np . array Numpy array containing the targets, in one-hot encoded format required model sklearn . base Trained sklearn multilabel classifier. required labels list List of labels for the categories to be predicted. required additional_features bool Whether or not FFT_q_standardised is included in data. Defaults to True. True path str Filename for the outputted file. Defaults to 'labels.xlsx'. 'labels.xlsx'","title":"write_model_preds()"},{"location":"reference/pxtextmining/factories/factory_write_results/#pxtextmining.factories.factory_write_results.write_model_analysis","text":"Writes an Excel file with the performance metrics of each label, as well as the counts of samples for each label. Parameters: Name Type Description Default model_name str Model name used in the performance metrics file required labels list List of labels for the categories to be predicted. required dataset pd . DataFrame Original dataset before train test split required path str Filepath where model and performance metrics file are saved. required","title":"write_model_analysis()"},{"location":"reference/pxtextmining/helpers/text_preprocessor/","text":"text_preprocessor tf_preprocessing ( X , max_sentence_length = 150 ) Conducts preprocessing with tensorflow tokenizer which vectorizes text and standardizes length. Parameters: Name Type Description Default X pd . Series Series containing the text to be processed required max_sentence_length int Maximum number of words. Defaults to 150. 150 Returns: Type Description tuple Tuple containing np.array of padded, tokenized, vectorized texts, and int showing number of unique words in vocabulary. Source code in pxtextmining/helpers/text_preprocessor.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def tf_preprocessing ( X , max_sentence_length = 150 ): \"\"\"Conducts preprocessing with tensorflow tokenizer which vectorizes text and standardizes length. Args: X (pd.Series): Series containing the text to be processed max_sentence_length (int, optional): Maximum number of words. Defaults to 150. Returns: (tuple): Tuple containing `np.array` of padded, tokenized, vectorized texts, and `int` showing number of unique words in vocabulary. \"\"\" tk = Tokenizer () tk . fit_on_texts ( X ) vocab_size = len ( tk . word_index ) print ( f 'There are { vocab_size } different words in your corpus' ) X_token = tk . texts_to_sequences ( X ) ### Pad the inputs X_pad = pad_sequences ( X_token , dtype = 'float32' , padding = 'post' , maxlen = max_sentence_length ) return X_pad , vocab_size","title":"Text preprocessor"},{"location":"reference/pxtextmining/helpers/text_preprocessor/#pxtextmining.helpers.text_preprocessor","text":"","title":"text_preprocessor"},{"location":"reference/pxtextmining/helpers/text_preprocessor/#pxtextmining.helpers.text_preprocessor.tf_preprocessing","text":"Conducts preprocessing with tensorflow tokenizer which vectorizes text and standardizes length. Parameters: Name Type Description Default X pd . Series Series containing the text to be processed required max_sentence_length int Maximum number of words. Defaults to 150. 150 Returns: Type Description tuple Tuple containing np.array of padded, tokenized, vectorized texts, and int showing number of unique words in vocabulary. Source code in pxtextmining/helpers/text_preprocessor.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def tf_preprocessing ( X , max_sentence_length = 150 ): \"\"\"Conducts preprocessing with tensorflow tokenizer which vectorizes text and standardizes length. Args: X (pd.Series): Series containing the text to be processed max_sentence_length (int, optional): Maximum number of words. Defaults to 150. Returns: (tuple): Tuple containing `np.array` of padded, tokenized, vectorized texts, and `int` showing number of unique words in vocabulary. \"\"\" tk = Tokenizer () tk . fit_on_texts ( X ) vocab_size = len ( tk . word_index ) print ( f 'There are { vocab_size } different words in your corpus' ) X_token = tk . texts_to_sequences ( X ) ### Pad the inputs X_pad = pad_sequences ( X_token , dtype = 'float32' , padding = 'post' , maxlen = max_sentence_length ) return X_pad , vocab_size","title":"tf_preprocessing()"},{"location":"reference/pxtextmining/helpers/tokenization/","text":"tokenization en_nlp = spacy . load ( 'en_core_web_lg' , disable = [ 'parser' , 'ner' ]) module-attribute spacy_tokenizer ( document ) Enables use of spacy tokenizer in the sklearn pipeline. Parameters: Name Type Description Default document str Text to be tokenized required Returns: Type Description list List containing tokenized, lemmatized words from input text. Source code in pxtextmining/helpers/tokenization.py 9 10 11 12 13 14 15 16 17 18 19 def spacy_tokenizer ( document ): \"\"\"Enables use of spacy tokenizer in the sklearn pipeline. Args: document (str): Text to be tokenized Returns: (list): List containing tokenized, lemmatized words from input text. \"\"\" doc_spacy = en_nlp ( document ) return [ token . lemma_ for token in doc_spacy ]","title":"Tokenization"},{"location":"reference/pxtextmining/helpers/tokenization/#pxtextmining.helpers.tokenization","text":"","title":"tokenization"},{"location":"reference/pxtextmining/helpers/tokenization/#pxtextmining.helpers.tokenization.en_nlp","text":"","title":"en_nlp"},{"location":"reference/pxtextmining/helpers/tokenization/#pxtextmining.helpers.tokenization.spacy_tokenizer","text":"Enables use of spacy tokenizer in the sklearn pipeline. Parameters: Name Type Description Default document str Text to be tokenized required Returns: Type Description list List containing tokenized, lemmatized words from input text. Source code in pxtextmining/helpers/tokenization.py 9 10 11 12 13 14 15 16 17 18 19 def spacy_tokenizer ( document ): \"\"\"Enables use of spacy tokenizer in the sklearn pipeline. Args: document (str): Text to be tokenized Returns: (list): List containing tokenized, lemmatized words from input text. \"\"\" doc_spacy = en_nlp ( document ) return [ token . lemma_ for token in doc_spacy ]","title":"spacy_tokenizer()"},{"location":"reference/pxtextmining/pipelines/multilabel_pipeline/","text":"multilabel_pipeline run_sklearn_pipeline ( additional_features = False , target = major_cats , models_to_try = [ 'mnb' , 'knn' , 'svm' , 'rfc' ], path = 'test_multilabel' , include_analysis = False ) Runs all the functions required to load multilabel data, preprocess it, and split it into training and test sets. Creates sklearn pipelines and hyperparameters to search, using specified estimators. For each estimator type selected, performs a randomized search across the hyperparameters to identify the parameters providing the best results on the holdout data within the randomized search. Evaluates the performance of the refitted estimator with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Parameters: Name Type Description Default additional_features bool Whether or not additional features (question type and text length) are used. Defaults to False. False target list The target labels, which should be columns in the dataset DataFrame. Defaults to major_cats. major_cats models_to_try list List of the estimators to try. Defaults to [\"mnb\", \"knn\", \"svm\", \"rfc\"]. Permitted values are \"mnb\" (Multinomial Naive Bayes), \"knn\" (K Nearest Neighbours), \"svm\" (Support Vector Classifier), or \"rfc\" (Random Forest Classifier). ['mnb', 'knn', 'svm', 'rfc'] path str Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. 'test_multilabel' run_svc_pipeline ( additional_features = False , target = major_cats , path = 'test_multilabel' , include_analysis = False ) Runs all the functions required to load multilabel data, preprocess it, and split it into training and test sets. Creates sklearn pipeline using a MultiOutputClassifier and Support Vector Classifier estimator, with specific hyperparameters. Fits the pipeline on the training data. Evaluates the performance of the refitted estimator with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder, together with optional further analysis in the form of Excel files. Parameters: Name Type Description Default additional_features bool Whether or not additional features (question type and text length) are used. Defaults to False. False target list The target labels, which should be columns in the dataset DataFrame. Defaults to major_cats. major_cats path str Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. 'test_multilabel' include_analysis bool Whether or not to create Excel files including further analysis of the model's performance. Defaults to False. If True, writes two Excel files to the specified folder, one containing the labels and the performance metrics for each label, and one containing the predicted labels and the actual labels for the test set, with the model's probabilities for both. False run_tf_pipeline ( target = major_cats , path = 'test_multilabel/tf' ) Runs all the functions required to load multilabel data, preprocess it, and split it into training and test sets. Creates tf.keras LSTM model and trains it on the train set. Evaluates the performance of trained model with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Cannot currently take additional features, is only designed for text data alone. This model architecture performs very poorly and may be taken out of the model. Parameters: Name Type Description Default target list The target labels, which should be columns in the dataset DataFrame. Defaults to major_cats. major_cats path str Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. 'test_multilabel/tf' run_bert_pipeline ( additional_features = False , path = 'test_multilabel/bert' , target = major_cats ) Runs all the functions required to load multilabel data, preprocess it, and split it into training, test and validation sets. Creates tf.keras Transformer model with additional layers specific to the classification task, and trains it on the train set. Evaluates the performance of trained model with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Parameters: Name Type Description Default additional_features bool Whether or not additional features (question type and text length) are used. Defaults to False. False path str Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. 'test_multilabel/bert'","title":"Multilabel pipeline"},{"location":"reference/pxtextmining/pipelines/multilabel_pipeline/#pxtextmining.pipelines.multilabel_pipeline","text":"","title":"multilabel_pipeline"},{"location":"reference/pxtextmining/pipelines/multilabel_pipeline/#pxtextmining.pipelines.multilabel_pipeline.run_sklearn_pipeline","text":"Runs all the functions required to load multilabel data, preprocess it, and split it into training and test sets. Creates sklearn pipelines and hyperparameters to search, using specified estimators. For each estimator type selected, performs a randomized search across the hyperparameters to identify the parameters providing the best results on the holdout data within the randomized search. Evaluates the performance of the refitted estimator with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Parameters: Name Type Description Default additional_features bool Whether or not additional features (question type and text length) are used. Defaults to False. False target list The target labels, which should be columns in the dataset DataFrame. Defaults to major_cats. major_cats models_to_try list List of the estimators to try. Defaults to [\"mnb\", \"knn\", \"svm\", \"rfc\"]. Permitted values are \"mnb\" (Multinomial Naive Bayes), \"knn\" (K Nearest Neighbours), \"svm\" (Support Vector Classifier), or \"rfc\" (Random Forest Classifier). ['mnb', 'knn', 'svm', 'rfc'] path str Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. 'test_multilabel'","title":"run_sklearn_pipeline()"},{"location":"reference/pxtextmining/pipelines/multilabel_pipeline/#pxtextmining.pipelines.multilabel_pipeline.run_svc_pipeline","text":"Runs all the functions required to load multilabel data, preprocess it, and split it into training and test sets. Creates sklearn pipeline using a MultiOutputClassifier and Support Vector Classifier estimator, with specific hyperparameters. Fits the pipeline on the training data. Evaluates the performance of the refitted estimator with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder, together with optional further analysis in the form of Excel files. Parameters: Name Type Description Default additional_features bool Whether or not additional features (question type and text length) are used. Defaults to False. False target list The target labels, which should be columns in the dataset DataFrame. Defaults to major_cats. major_cats path str Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. 'test_multilabel' include_analysis bool Whether or not to create Excel files including further analysis of the model's performance. Defaults to False. If True, writes two Excel files to the specified folder, one containing the labels and the performance metrics for each label, and one containing the predicted labels and the actual labels for the test set, with the model's probabilities for both. False","title":"run_svc_pipeline()"},{"location":"reference/pxtextmining/pipelines/multilabel_pipeline/#pxtextmining.pipelines.multilabel_pipeline.run_tf_pipeline","text":"Runs all the functions required to load multilabel data, preprocess it, and split it into training and test sets. Creates tf.keras LSTM model and trains it on the train set. Evaluates the performance of trained model with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Cannot currently take additional features, is only designed for text data alone. This model architecture performs very poorly and may be taken out of the model. Parameters: Name Type Description Default target list The target labels, which should be columns in the dataset DataFrame. Defaults to major_cats. major_cats path str Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. 'test_multilabel/tf'","title":"run_tf_pipeline()"},{"location":"reference/pxtextmining/pipelines/multilabel_pipeline/#pxtextmining.pipelines.multilabel_pipeline.run_bert_pipeline","text":"Runs all the functions required to load multilabel data, preprocess it, and split it into training, test and validation sets. Creates tf.keras Transformer model with additional layers specific to the classification task, and trains it on the train set. Evaluates the performance of trained model with the best hyperparameters on the test set, and saves the model and the performance metrics to a specified folder. Parameters: Name Type Description Default additional_features bool Whether or not additional features (question type and text length) are used. Defaults to False. False path str Path where the models are to be saved. If path does not exist, it will be created. Defaults to 'test_multilabel'. 'test_multilabel/bert'","title":"run_bert_pipeline()"}]}